{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a1cb6d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import glob\n",
    "import shutil\n",
    "import seaborn as sns\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aafbff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.feature_selection import RFE, SelectKBest, mutual_info_classif,f_classif\n",
    "from sklearn.model_selection import (KFold, train_test_split, cross_validate, GridSearchCV, RepeatedStratifiedKFold,\n",
    "                                     cross_val_score, GroupKFold, StratifiedGroupKFold, StratifiedKFold)\n",
    "from sklearn.metrics import (roc_auc_score, recall_score, make_scorer, f1_score, confusion_matrix, roc_curve, accuracy_score,\n",
    "                             precision_recall_curve, auc, plot_precision_recall_curve)\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis                              \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from feature_engine.selection import RecursiveFeatureAddition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaeea1a",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9118024",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40f10fa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialize the file paths to hold all the images separated by sequence and SRS Date\n",
    "root = '/Users/cxl037/PycharmProjects/pythonProject1/Example_Data/'\n",
    "root_ML = '/Users/cxl037/PycharmProjects/DeepLearning'\n",
    "model_path = os.path.join(root_ML, 'Models', 'Post-Pre')\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)\n",
    "PreT1Bravo_path = os.path.join(root, 'PreT1Bravo_Immuno')\n",
    "PreT1Bravo_batchfile = os.path.join(PreT1Bravo_path, 'radiomics_features_re.csv')\n",
    "PreT1Vasc_path = os.path.join(root, 'PreT1Vasc_Immuno')\n",
    "PreT1Vasc_batchfile = os.path.join(PreT1Vasc_path, 'radiomics_features_re.csv')\n",
    "PreT2Flair_path = os.path.join(root, 'PreT2Flair_Immuno')\n",
    "PreT2Flair_batchfile = os.path.join(PreT2Flair_path, 'radiomics_features_re.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f30831d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read T1Bravo data and add T1_ prefix to all the feature columnsextractedFeatures = pd.read_csv(PreT1Bravo_batchfile)\n",
    "extractedFeatures = extractedFeatures.add_prefix('Pre_T1_')\n",
    "extractedFeatures.rename(columns = {'Pre_T1_Image': 'T1_Image', 'Pre_T1_Mask': 'Mask'}, inplace = True)\n",
    "# Get patientID for future merging\n",
    "extractedFeatures['PatientID'] = extractedFeatures.apply(lambda row: row['Mask'][:7], axis=1)\n",
    "extractedFeatures['PatientID'] = extractedFeatures['PatientID'].astype(str)\n",
    "extractedFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb6f9c4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read T1Vasc data and add Vasc_ prefix to all the feature columns\n",
    "extractedFeatures2 = pd.read_csv(PreT1Vasc_batchfile)\n",
    "extractedFeatures2 = extractedFeatures2.add_prefix('Pre_Vasc_')\n",
    "extractedFeatures2.rename(columns = {'Pre_Vasc_Image': 'Vasc_Image', 'Pre_Vasc_Mask': 'Mask'}, inplace = True)\n",
    "extractedFeatures2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62c09a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read T2Flair data and add T2_ prefix to all the feature columns\n",
    "extractedFeatures3 = pd.read_csv(PreT2Flair_batchfile)\n",
    "extractedFeatures3 = extractedFeatures3.add_prefix('Pre_T2_')\n",
    "extractedFeatures3.rename(columns = {'Pre_T2_Image': 'T2_Image', 'Pre_T2_Mask': 'Mask'}, inplace = True)\n",
    "extractedFeatures3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e010a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge T1Bravo, T1Vasc, and T2Flair columns together\n",
    "allFeatures = pd.merge(extractedFeatures, extractedFeatures2, on='Mask', how = 'left' )\n",
    "allFeatures = pd.merge(allFeatures, extractedFeatures3, on='Mask', how = 'left' )\n",
    "allFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bba0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get patient immunotherapy information\n",
    "patientDetails = pd.read_excel('/Users/cxl037/PycharmProjects/pythonProject1/SRS_immune_list.xlsx')\n",
    "patientDetails['PatientID'] = patientDetails.apply(lambda row: str(row['MRN']) if len(str(row['MRN'])) == 7 else '0' + str(row['MRN']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d8574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use patientID to merge radiomic_features.csv and SRS_immune_list.xlsx\n",
    "allFeatures = pd.merge(allFeatures, patientDetails, on='PatientID', how = 'left')\n",
    "allFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e08e09",
   "metadata": {},
   "source": [
    "### Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f7efc3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Drop all diagnostic features\n",
    "keep = ['Immunotherapy_prior_3_months', 'PatientID', 'Mask']\n",
    "remove_cols = [feature for feature in allFeatures.columns \n",
    "               if not (feature.startswith(\"Pre_T2_original\") or feature.startswith(\"Pre_T1_original\") \n",
    "               or feature.startswith(\"Pre_Vasc_original\") or feature.startswith(\"Post_T1_original\")\n",
    "               or feature.startswith(\"Post_Vasc_original\") or feature.startswith(\"Post_T2_original\")\n",
    "               or feature in keep)]\n",
    "filteredFeatures = allFeatures.drop(remove_cols, axis = 1)\n",
    "filteredFeatures = filteredFeatures.dropna() # remove rows with NA values\n",
    "filteredFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4de108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get patient complete response information\n",
    "cr = pd.read_excel('/Users/cxl037/PycharmProjects/pythonProject1/SRS_immuno_CR.xlsx')\n",
    "cr['Mask'] = cr.apply(lambda row: row['Mask'].strip(), axis=1)\n",
    "# display(cr['Mask'])\n",
    "# display(filteredFeatures['Mask'])\n",
    "\n",
    "# Use Mask to merge features and SRS_immune_CR.xlsx\n",
    "filteredFeatures = pd.merge(filteredFeatures, cr, on='Mask', how = 'left')\n",
    "filteredFeatures = filteredFeatures.dropna()\n",
    "filteredFeatures['CR'] = filteredFeatures['CR'].astype(int)\n",
    "filteredFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e38fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out all the small lesions less than a certain diameter\n",
    "filteredFeatures = filteredFeatures.loc[filteredFeatures['Pre_T1_original_shape_Maximum3DDiameter'] >= 10]\n",
    "with pd.option_context('display.max_rows', None):  # more options can be specified also\n",
    "    display(filteredFeatures['Pre_T1_original_shape_Maximum3DDiameter'])\n",
    "    display(filteredFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12289720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data and labels\n",
    "X = filteredFeatures.drop(['CR', 'PatientID', 'Mask', 'Immunotherapy_prior_3_months'], axis=1)\n",
    "y = filteredFeatures['CR']\n",
    "feature_names = list(X.columns)\n",
    "allSubjects = list(filteredFeatures.loc[:, 'PatientID'])\n",
    "rd=42\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c74680",
   "metadata": {},
   "source": [
    "### Removing Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05670a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def get_correlated_features(df, threshold=0.95, max_pvalue=0.05):\n",
    "    corr_matrix = np.zeros((df.shape[1], df.shape[1]))\n",
    "    pvalue_matrix = np.zeros((df.shape[1], df.shape[1]))\n",
    "    msk_cols = list(df.columns)\n",
    "\n",
    "    # initializes (i,j) with the correlation coefficient and p-value for testing non-correlation \n",
    "    # between columns i and j\n",
    "    for i in range(df.shape[1]):\n",
    "        for j in range(df.shape[1]):\n",
    "            corrtest = pearsonr(df[df.columns[i]], df[df.columns[j]])\n",
    "            corr_matrix[i, j] = corrtest[0]\n",
    "            pvalue_matrix[i, j] = corrtest[1]\n",
    "    \n",
    "    p_values = []\n",
    "    for i in range(df.shape[1]):\n",
    "        for j in range(i + 1, df.shape[1]):\n",
    "            p_values.append(pvalue_matrix[i, j])\n",
    "    \n",
    "    # corrected p-values to make sure that no false significant results occur\n",
    "    p_values_corrected = fdrcorrection(p_values, alpha=0.05, method='indep', is_sorted=False)[1]\n",
    "    pvalues_corrected_matrix = np.zeros((df.shape[1], df.shape[1]))\n",
    "    \n",
    "\n",
    "    k = 0\n",
    "    for i in range(df.shape[1]):\n",
    "        for j in range(i + 1, df.shape[1]):\n",
    "            pvalues_corrected_matrix[i, j] = p_values_corrected[k]\n",
    "            pvalues_corrected_matrix[j, i] = p_values_corrected[k]\n",
    "            k += 1\n",
    "\n",
    "    to_drop_matrix = np.zeros((df.shape[1], df.shape[1]))\n",
    "    \n",
    "    # Create a matrix where (i, j) is correlated but j > i, in other words only consider upper triangular indices\n",
    "    for i in range(df.shape[1]):\n",
    "        for j in range(i + 1, df.shape[1]):\n",
    "            if pvalues_corrected_matrix[i, j] < max_pvalue and abs(corr_matrix[i, j]) > threshold:\n",
    "                to_drop_matrix[i, j] = 1\n",
    "            else:\n",
    "                if abs(corr_matrix[i, j]) > threshold:\n",
    "                    print(msk_cols[i] + \" * \" + msk_cols[j] + 'corr, pvalue, fdr: %f, %f, %f' % (\n",
    "                        np.round(corr_matrix[i, j], decimals=3),\n",
    "                        np.round(pvalue_matrix[i, j], decimals=3),\n",
    "                        np.round(pvalues_corrected_matrix[i, j], decimals=3)))\n",
    "                to_drop_matrix[i, j] = 0\n",
    "\n",
    "    upper = pd.DataFrame(to_drop_matrix)\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] == 1)]\n",
    "    \n",
    "    correlated_feats = {}\n",
    "    for feature in msk_cols:\n",
    "        correlated_feats[feature] = set()\n",
    "    for i in to_drop:\n",
    "        for j in upper.columns:\n",
    "            if upper[i][j] > threshold:\n",
    "                correlated_feats[msk_cols[j]].add(msk_cols[i]) # adds the dropped features as the value\n",
    "\n",
    "    feats_to_drop = [msk_cols[i] for i in to_drop]\n",
    "\n",
    "    # show how the kept features correlate with the dropped features\n",
    "    new_correlated_feats = {}\n",
    "    for feat in correlated_feats.keys():\n",
    "        if feat not in feats_to_drop and len(correlated_feats[feat]) > 0:\n",
    "            new_correlated_feats[feat] = correlated_feats[feat]\n",
    "    correlated_feats = new_correlated_feats\n",
    "\n",
    "\n",
    "    printDict = {'corr_matrix': corr_matrix,\n",
    "                 'p_values_matrix': pvalue_matrix,\n",
    "                 'p_values_corrected_matrix': pvalues_corrected_matrix,\n",
    "                 'correlated_feats': correlated_feats,\n",
    "                 'feats_to_drop': feats_to_drop\n",
    "                }\n",
    "    return printDict\n",
    "    \n",
    "\n",
    "result = get_correlated_features(X)\n",
    "\n",
    "\n",
    "corr_matrix = abs(result['corr_matrix'])\n",
    "corr_pvalues = result['p_values_matrix']\n",
    "corr_pvalues_corrected = result['p_values_corrected_matrix']\n",
    "correlated_feats = result['correlated_feats']\n",
    "feats_to_drop = result['feats_to_drop']\n",
    "\n",
    "print('feats_to_drop',feats_to_drop)\n",
    "print(len(feats_to_drop))\n",
    "print('\\n')\n",
    "print('correlated_feats',correlated_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1233d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all correlated features\n",
    "X.drop(feats_to_drop, axis = 1, inplace=True)\n",
    "feature_names = list(X.columns)\n",
    "# fig, ax = plt.subplots(figsize=(30,30))\n",
    "# ax = sns.heatmap(X.corr())\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd13292",
   "metadata": {},
   "source": [
    "## Analyzing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ee7ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into no complete response and complete response\n",
    "no_cr = X.loc[y==0]\n",
    "cr = X.loc[y==1]\n",
    "display(no_cr)\n",
    "display(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe8e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run t-test on each of the features to see if there is a difference in no CR and CR\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "significant_features = []\n",
    "p_values = []\n",
    "count = 0\n",
    "for feature in cr.columns:\n",
    "    t_stat, pvalue = stats.ttest_ind(cr[feature], no_cr[feature], equal_var=True)\n",
    "    p_values.append(pvalue)\n",
    "p_values_corrected = fdrcorrection(p_values, alpha=0.05, method='indep', is_sorted=False)[1]\n",
    "for i in range(len(p_values)):\n",
    "    if p_values_corrected[i] <= 0.05:\n",
    "        print(cr.columns[i])\n",
    "        count += 1\n",
    "print(count)\n",
    "# p_values_notsorted = p_values_corrected.copy()\n",
    "# p_values_corrected.sort(reverse=True)\n",
    "# for i in p_values_corrected:\n",
    "#     print(cr.columns[p_values_notsorted.index(i)])\n",
    "    \n",
    "# sig_features = []\n",
    "# for i in range(len(p_values)):\n",
    "#     if p_values_corrected[i] <= 0.05:\n",
    "#         sig_features.append(cr.columns[i])\n",
    "# print(sig_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3388ea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Analysis\n",
    "sc = StandardScaler()\n",
    "X_train_ori = X\n",
    "y_train_ori = y\n",
    "X_train = sc.fit_transform(X)\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_train.columns = X.columns\n",
    "X_train\n",
    "\n",
    "lr = LogisticRegression()\n",
    "AUCforUniLogit = {}\n",
    "for (featureName, featureData) in X_train.iteritems():\n",
    "    lr = lr.fit(featureData.values.reshape(-1, 1),y)\n",
    "    roc_auc = roc_auc_score(y, lr.predict_proba(featureData.values.reshape(-1, 1))[:, 1])\n",
    "    AUCforUniLogit[featureName] = roc_auc\n",
    "AUCforUniLogitTop = dict(sorted(AUCforUniLogit.items(), key=lambda item: item[1], reverse=True))\n",
    "AUCforUniLogitRank = list(AUCforUniLogitTop.keys())\n",
    "AUCforUniLogitTop20 = dict(sorted(AUCforUniLogit.items(), key=lambda item: item[1], reverse=True)[:20])\n",
    "print(\"{:<50} {:<50}\".format('feature','auc value'))\n",
    "for key, value in AUCforUniLogitTop20.items():\n",
    "    print(\"{:<50} {:<50}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f4a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.lines as mlines\n",
    "# feature = 'Pre_T1_original_glszm_SizeZoneNonUniformity'\n",
    "# df1 = cr[feature]\n",
    "# df2 = no_cr[feature]\n",
    "# plt.plot(df1, len(df1) * [1], 'x')\n",
    "# plt.plot(df2, len(df2) * [2], 'o')\n",
    "# plt.title('T1_SizeZoneNonUniformity')\n",
    "# markers = [mlines.Line2D([], [], color='orange', marker='o', markersize=5, label='No CR'),\n",
    "#           mlines.Line2D([], [], color='blue', marker='x', markersize=5, label='CR')]\n",
    "# plt.legend(handles=markers, loc='best')\n",
    "# plt.show()\n",
    "# t_stat, pvalue = stats.ttest_ind(cr[feature], no_cr[feature], equal_var=True)\n",
    "# print(\"t-value: \", t_stat)\n",
    "# print(\"pvalue: \", pvalue)\n",
    "# print('CR mean: ', cr[feature].mean())\n",
    "# print('No CR mean: ', no_cr[feature].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a4562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# immuno = filteredFeatures.loc[filteredFeatures['Immunotherapy_prior_3_months'] == 1]\n",
    "# cr_immuno = immuno.loc[immuno['CR'] == 1]\n",
    "# nocr_immuno = immuno.loc[immuno['CR'] == 0]\n",
    "# CR_immuno = len(cr_immuno)\n",
    "# immuno_length = len(immuno)\n",
    "# volume = immuno['Pre_T1_original_shape_MeshVolume'].sum()\n",
    "# cr_volume = cr_immuno['Pre_T1_original_shape_MeshVolume'].sum()\n",
    "# nocr_volume = nocr_immuno['Pre_T1_original_shape_MeshVolume'].sum()\n",
    "# print(\"#CR in immuno cohort: \", CR_immuno)\n",
    "# print('#lesions in immuno cohort: ', immuno_length)\n",
    "# print('Average volume overall: ', volume/immuno_length)\n",
    "# print('ratio: ', CR_immuno/immuno_length)\n",
    "# print('Average volume in CR cohort: ', cr_volume/CR_immuno)\n",
    "# print('Average volume in not CR cohort: ', nocr_volume/(immuno_length - CR_immuno))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2f7327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_immuno = filteredFeatures.loc[filteredFeatures['Immunotherapy_prior_3_months'] == 0]\n",
    "# cr_no_immuno = no_immuno.loc[no_immuno['CR'] == 1]\n",
    "# nocr_no_immuno = no_immuno.loc[no_immuno['CR'] == 0]\n",
    "# CR_no = len(cr_no_immuno)\n",
    "# no_length = len(no_immuno)\n",
    "# volume = no_immuno['Pre_T1_original_shape_MeshVolume'].sum()\n",
    "# cr_volume = cr_no_immuno['Pre_T1_original_shape_MeshVolume'].sum()\n",
    "# nocr_volume = nocr_no_immuno['Pre_T1_original_shape_MeshVolume'].sum()\n",
    "# print(\"#CR in no immuno cohort: \", CR_no)\n",
    "# print('#lesions in no immuno cohort: ', no_length)\n",
    "# print('Average volume overall: ', volume/no_length)\n",
    "# print('Average volume in CR cohort: ', cr_volume/CR_no)\n",
    "# print('Average volume in not CR cohort: ', nocr_volume/(no_length - CR_no))\n",
    "# print('ratio: ', CR_no/no_length)\n",
    "# # immuno = filteredFeatures.loc[filteredFeatures['Immunotherapy_prior_3_months'] == 1]\n",
    "# # immuno = immuno.drop(['Immunotherapy_prior_3_months', 'PatientID'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f090f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_cr = filteredFeatures.loc[filteredFeatures['CR']==0]\n",
    "# cr = filteredFeatures.loc[filteredFeatures['CR']==1]\n",
    "# with pd.option_context('display.max_rows', None):  # more options can be specified also\n",
    "#     display(no_cr['Pre_T1_original_shape_Maximum3DDiameter'])\n",
    "#     display(cr['Pre_T1_original_shape_Maximum3DDiameter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf280d3",
   "metadata": {},
   "source": [
    "## Running Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ced3922",
   "metadata": {},
   "source": [
    "### Defining variables and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b996c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE\n",
    "from collections import Counter\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks, NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc32e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "N_SPLITS = 3\n",
    "N_REPEATS = 10\n",
    "cv = RepeatedStratifiedKFold(n_splits=N_SPLITS, n_repeats=N_REPEATS, random_state=rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a91c799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(clf, cv, resample=None):\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    aucpr_scores = []\n",
    "    for fold, (train_idxs, test_idxs) in enumerate(cv.split(X, y, groups=allSubjects)):\n",
    "        print(\"Repeat :\", fold // N_SPLITS + 1)\n",
    "        print(\"Fold :\", fold % N_SPLITS + 1)\n",
    "#         print(\"train length:\", len(train_idxs))\n",
    "#         print(\"test length:\", len(test_idxs))\n",
    "        \n",
    "        # split data\n",
    "        X_train, X_test = X.iloc[train_idxs], X.iloc[test_idxs]\n",
    "        y_train, y_test = y.iloc[train_idxs], y.iloc[test_idxs]\n",
    "        counter = Counter(y_train)\n",
    "        print(counter)\n",
    "        \n",
    "        # resample if needed\n",
    "        if resample:\n",
    "            X_train, y_train = resample.fit_resample(X_train, y_train)\n",
    "            counter = Counter(y_train)\n",
    "            print(counter)\n",
    "            \n",
    "        # fit model\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # calculate scores\n",
    "        train_auc_score = roc_auc_score(y_train, clf.predict_proba(X_train)[:, 1])\n",
    "        print(\"TRAIN SCORE: \", train_auc_score)\n",
    "        train_scores.append(train_auc_score)\n",
    "        \n",
    "        test_auc_score = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "        test_scores.append(test_auc_score)\n",
    "        print(\"TEST AUC SCORE: \", test_auc_score)\n",
    "        \n",
    "        precision, recall, thresholds = precision_recall_curve(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "        auc_precision_recall = auc(recall, precision)\n",
    "        aucpr_scores.append(auc_precision_recall)\n",
    "        print(\"TEST PR AUC SCORE: \",auc_precision_recall)\n",
    "        \n",
    "        #     predictions = rfc.predict_proba(X_test)[:, 1]\n",
    "        #     actual = list(y_test)\n",
    "        #     print('Pred: ', predictions)\n",
    "        #     print('Actual: ', actual) \n",
    "        \n",
    "        # AUC graph\n",
    "        #     fpr, tpr, _ = roc_curve(y_test, rfc.predict_proba(X_test)[:, 1])\n",
    "        #     plt.plot(fpr, tpr, marker='.', label='Random Forest')\n",
    "        #     plt.xlabel('False Positive Rate')\n",
    "        #     plt.ylabel('True Positive Rate')\n",
    "        #     # show the legend\n",
    "        #     plt.legend()\n",
    "        #     # show the plot\n",
    "        #     plt.show()\n",
    "    \n",
    "    # averages across all fold\n",
    "    print(\"\")\n",
    "    print(\"MEAN TRAIN SCORES:\", np.mean(train_scores))\n",
    "    print(\"STD TRAIN SCORES:\", np.std(train_scores))\n",
    "    print(\"MEAN TEST SCORES:\", np.mean(test_scores))\n",
    "    print(\"STD TEST SCORES:\", np.std(test_scores))\n",
    "    print(\"MEAN AUCPR TEST SCORES:\", np.mean(aucpr_scores))\n",
    "    print(\"STD AUCPR TEST SCORES:\", np.std(aucpr_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbf0dc2",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fdbbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(objective=\"binary:logistic\", random_state=rd, eval_metric='auc',use_label_encoder=False,\n",
    "                   max_depth=2, gamma=0, min_child_weight=1, reg_lambda=2, colsample_bytree=1)\n",
    "\n",
    "cross_val(xgb, cv, SMOTE())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afab803",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31b7f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators = 300, max_features=2, max_depth = 1, random_state=rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724045db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val(rfc, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a41c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_balanced = RandomForestClassifier(n_estimators = 300, max_features=2, max_depth = 1, random_state=rd, class_weight='balanced')\n",
    "cross_val(rfc_balanced, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7195e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val(rfc, cv, resample=SMOTE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bdd92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val(rfc, cv, resample=TomekLinks(sampling_strategy='majority'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1fc166",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val(rfc, cv, resample=NearMiss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769e9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Important Features from RF\n",
    "rfc = RandomForestClassifier(n_estimators = 300, max_features='sqrt', max_depth = 1, random_state=rd)\n",
    "rfc.fit(X, y)\n",
    "sort = rfc.feature_importances_.argsort()[-21: -1]\n",
    "plt.barh([feature_names[s] for s in sort], rfc.feature_importances_[sort])\n",
    "plt.xlabel(\"Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d380d0",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee714a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l2', C=10, solver='liblinear')\n",
    "cross_val(lr, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d067f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val(lr, cv, SMOTE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab55163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting importance coefficients from Lasso\n",
    "lr.fit(X, y)\n",
    "coefficients = lr.coef_\n",
    "importance = list(np.abs(coefficients).reshape(-1))\n",
    "posImportance = sorted([i for i in importance if i > 0])\n",
    "for i in posImportance:\n",
    "    print(feature_names[importance.index(i)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c718998",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4344d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(C=0.005, kernel='linear', probability=True)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=rd)\n",
    "cross_val(svc, cv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
