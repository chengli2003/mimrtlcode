{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a1cb6d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import glob\n",
    "import shutil\n",
    "import seaborn as sns\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa88f1c1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.feature_selection import RFE, SelectKBest, mutual_info_classif,f_classif\n",
    "from sklearn.model_selection import (KFold, train_test_split, cross_validate, GridSearchCV, RepeatedStratifiedKFold,\n",
    "                                     cross_val_score, GroupKFold, StratifiedGroupKFold, StratifiedKFold)\n",
    "from sklearn.metrics import roc_auc_score, recall_score, make_scorer, f1_score, confusion_matrix, roc_curve, accuracy_score\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis                              \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from feature_engine.selection import RecursiveFeatureAddition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c44bb93",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7908c8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialize the file paths to hold all the images separated by sequence and SRS Date\n",
    "root = '/Users/cxl037/PycharmProjects/pythonProject1/Example_Data/'\n",
    "PreT1Bravo_path = os.path.join(root, 'PreT1Bravo_Immuno')\n",
    "PreT1Bravo_batchfile = os.path.join(PreT1Bravo_path, 'radiomics_features_re.csv')\n",
    "PreT1Vasc_path = os.path.join(root, 'PreT1Vasc_Immuno')\n",
    "PreT1Vasc_batchfile = os.path.join(PreT1Vasc_path, 'radiomics_features_re.csv')\n",
    "PreT2Flair_path = os.path.join(root, 'PreT2Flair_Immuno')\n",
    "PreT2Flair_batchfile = os.path.join(PreT2Flair_path, 'radiomics_features_re.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2af5814",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read T1Bravo data and add T1_ prefix to all the feature columns\n",
    "extractedFeatures = pd.read_csv(PreT1Bravo_batchfile)\n",
    "extractedFeatures = extractedFeatures.add_prefix('T1_')\n",
    "extractedFeatures.rename(columns = {'T1_Mask': 'Mask'}, inplace = True)\n",
    "# Get patientID for future merging\n",
    "extractedFeatures['PatientID'] = extractedFeatures.apply(lambda row: row['Mask'][:7], axis=1)\n",
    "extractedFeatures['PatientID'] = extractedFeatures['PatientID'].astype(str)\n",
    "extractedFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b99a5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read T1Vasc data and add Vasc_ prefix to all the feature columns\n",
    "extractedFeatures2 = pd.read_csv(PreT1Vasc_batchfile)\n",
    "extractedFeatures2 = extractedFeatures2.add_prefix('Vasc_')\n",
    "extractedFeatures2.rename(columns = {'Vasc_Mask': 'Mask'}, inplace = True)\n",
    "extractedFeatures2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087537cd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read T2Flair data and add T2_ prefix to all the feature columns\n",
    "extractedFeatures3 = pd.read_csv(PreT2Flair_batchfile)\n",
    "extractedFeatures3 = extractedFeatures3.add_prefix('T2_')\n",
    "extractedFeatures3.rename(columns = {'T2_Mask': 'Mask'}, inplace = True)\n",
    "extractedFeatures3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7957d828",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Merge T1Bravo, T1Vasc, and T2Flair columns together\n",
    "allFeatures = pd.merge(extractedFeatures, extractedFeatures2, on='Mask', how = 'left' )\n",
    "allFeatures = pd.merge(allFeatures, extractedFeatures3, on='Mask', how = 'left' )\n",
    "allFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cfd07c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get patient immunotherapy information\n",
    "patientDetails = pd.read_excel('/Users/cxl037/PycharmProjects/pythonProject1/SRS_immune_list.xlsx')\n",
    "patientDetails['PatientID'] = patientDetails.apply(lambda row: str(row['MRN']) if len(str(row['MRN'])) == 7 else '0' + str(row['MRN']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793f9228",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Use patientID to merge radiomic_features.csv and SRS_immune_list.xlsx\n",
    "allFeatures = pd.merge(allFeatures, patientDetails, on='PatientID', how = 'left')\n",
    "allFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43813f3a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Drop all diagnostic features\n",
    "remove_cols = [feature for feature in allFeatures.columns \n",
    "               if not (feature.startswith(\"T2_original\") or feature.startswith(\"T1_original\") or feature.startswith(\"Vasc_original\") \n",
    "               or feature == 'Immunotherapy_prior_3_months' or feature == 'PatientID')]\n",
    "filteredFeatures = allFeatures.drop(remove_cols, axis = 1)\n",
    "# remove rows with NA values, only preserving those rows which have information for all sequences\n",
    "filteredFeatures = filteredFeatures.dropna()\n",
    "# Get all the subject ids (useful for RepeatedStratifiedGroupKFold)\n",
    "allSubjects = list(filteredFeatures.loc[:, 'PatientID'])\n",
    "print(len(set(allSubjects)))\n",
    "print(len(allSubjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74edc55",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rd=1\n",
    "# Get training data and labels\n",
    "X = filteredFeatures.drop(['Immunotherapy_prior_3_months', 'PatientID'], axis=1)\n",
    "y = filteredFeatures['Immunotherapy_prior_3_months']\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec743b6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986ac44c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Normalizing data\n",
    "sc = StandardScaler()\n",
    "X_train_ori = X\n",
    "y_train_ori = y\n",
    "X_norm = sc.fit_transform(X)\n",
    "X_norm = pd.DataFrame(X_norm)\n",
    "X_norm.columns = X.columns\n",
    "X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3601044",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Order features by highest univariate AUC\n",
    "lr = LogisticRegression()\n",
    "AUCforUniLogit = {}\n",
    "for (featureName, featureData) in X_norm.iteritems():\n",
    "    lr = lr.fit(featureData.values.reshape(-1, 1),y)\n",
    "    auc = roc_auc_score(y, lr.predict_proba(featureData.values.reshape(-1, 1))[:, 1])\n",
    "    AUCforUniLogit[featureName] = auc\n",
    "AUCforUniLogitTop = dict(sorted(AUCforUniLogit.items(), key=lambda item: item[1], reverse=True))\n",
    "AUCforUniLogitRank = list(AUCforUniLogitTop.keys())\n",
    "AUCforUniLogitTop20 = dict(sorted(AUCforUniLogit.items(), key=lambda item: item[1], reverse=True)[:20])\n",
    "print(\"{:<50} {:<50}\".format('feature','auc value'))\n",
    "for key, value in AUCforUniLogitTop20.items():\n",
    "    print(\"{:<50} {:<50}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d0de72",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Removing Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14d6ed3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot correlation heatmap before\n",
    "fig, ax = plt.subplots(figsize=(30,30))\n",
    "ax = sns.heatmap(X_norm.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f169d04d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def get_correlated_features(df, threshold=0.95, max_pvalue=0.05):\n",
    "    corr_matrix = np.zeros((df.shape[1], df.shape[1]))\n",
    "    pvalue_matrix = np.zeros((df.shape[1], df.shape[1]))\n",
    "    msk_cols = list(df.columns)\n",
    "\n",
    "    # initializes (i,j) with the correlation coefficient and p-value for testing non-correlation \n",
    "    # between columns i and j\n",
    "    for i in range(df.shape[1]):\n",
    "        for j in range(df.shape[1]):\n",
    "            corrtest = pearsonr(df[df.columns[i]], df[df.columns[j]])\n",
    "            corr_matrix[i, j] = corrtest[0]\n",
    "            pvalue_matrix[i, j] = corrtest[1]\n",
    "    \n",
    "    p_values = []\n",
    "    for i in range(df.shape[1]):\n",
    "        for j in range(i + 1, df.shape[1]):\n",
    "            p_values.append(pvalue_matrix[i, j])\n",
    "    \n",
    "    # corrected p-values to make sure that no false significant results occur\n",
    "    p_values_corrected = fdrcorrection(p_values, alpha=0.05, method='indep', is_sorted=False)[1]\n",
    "    pvalues_corrected_matrix = np.zeros((df.shape[1], df.shape[1]))\n",
    "    \n",
    "\n",
    "    k = 0\n",
    "    for i in range(df.shape[1]):\n",
    "        for j in range(i + 1, df.shape[1]):\n",
    "            pvalues_corrected_matrix[i, j] = p_values_corrected[k]\n",
    "            pvalues_corrected_matrix[j, i] = p_values_corrected[k]\n",
    "            k += 1\n",
    "\n",
    "    to_drop_matrix = np.zeros((df.shape[1], df.shape[1]))\n",
    "    \n",
    "    # Create a matrix where (i, j) is correlated but j > i, in other words only consider upper triangular indices\n",
    "    for i in range(df.shape[1]):\n",
    "        for j in range(i + 1, df.shape[1]):\n",
    "            if pvalues_corrected_matrix[i, j] < max_pvalue and abs(corr_matrix[i, j]) > threshold:\n",
    "                to_drop_matrix[i, j] = 1\n",
    "            else:\n",
    "                if abs(corr_matrix[i, j]) > threshold:\n",
    "                    print(msk_cols[i] + \" * \" + msk_cols[j] + 'corr, pvalue, fdr: %f, %f, %f' % (\n",
    "                        np.round(corr_matrix[i, j], decimals=3),\n",
    "                        np.round(pvalue_matrix[i, j], decimals=3),\n",
    "                        np.round(pvalues_corrected_matrix[i, j], decimals=3)))\n",
    "                to_drop_matrix[i, j] = 0\n",
    "\n",
    "    upper = pd.DataFrame(to_drop_matrix)\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] == 1)]\n",
    "    \n",
    "    correlated_feats = {}\n",
    "    for feature in msk_cols:\n",
    "        correlated_feats[feature] = set()\n",
    "    for i in to_drop:\n",
    "        for j in upper.columns:\n",
    "            if upper[i][j] > threshold:\n",
    "                correlated_feats[msk_cols[j]].add(msk_cols[i]) # adds the dropped features as the value\n",
    "\n",
    "    feats_to_drop = [msk_cols[i] for i in to_drop]\n",
    "\n",
    "    # show how the kept features correlate with the dropped features\n",
    "    new_correlated_feats = {}\n",
    "    for feat in correlated_feats.keys():\n",
    "        if feat not in feats_to_drop and len(correlated_feats[feat]) > 0:\n",
    "            new_correlated_feats[feat] = correlated_feats[feat]\n",
    "    correlated_feats = new_correlated_feats\n",
    "\n",
    "\n",
    "    printDict = {'corr_matrix': corr_matrix,\n",
    "                 'p_values_matrix': pvalue_matrix,\n",
    "                 'p_values_corrected_matrix': pvalues_corrected_matrix,\n",
    "                 'correlated_feats': correlated_feats,\n",
    "                 'feats_to_drop': feats_to_drop\n",
    "                }\n",
    "    return printDict\n",
    "    \n",
    "\n",
    "result = get_correlated_features(X)\n",
    "\n",
    "\n",
    "corr_matrix = abs(result['corr_matrix'])\n",
    "corr_pvalues = result['p_values_matrix']\n",
    "corr_pvalues_corrected = result['p_values_corrected_matrix']\n",
    "correlated_feats = result['correlated_feats']\n",
    "feats_to_drop = result['feats_to_drop']\n",
    "\n",
    "print('feats_to_drop',feats_to_drop)\n",
    "print(len(feats_to_drop))\n",
    "print('\\n')\n",
    "print('correlated_feats',correlated_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7261db6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Drop all correlated features\n",
    "X_uncorr = X.drop(feats_to_drop, axis = 1)\n",
    "X_train = X_uncorr.copy()\n",
    "display(X_train)\n",
    "y_train = y.copy()\n",
    "feature_names = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6014d585",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Heatmap with reduced features\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30,30))\n",
    "ax = sns.heatmap(X_train.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae8dcc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Defining Functions and Variables for ML Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58697a0b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(\n",
    "    estimator,\n",
    "    title,\n",
    "    X,\n",
    "    y,\n",
    "    scoring,\n",
    "    groups=None,\n",
    "    axes=None,\n",
    "    ylim=None,\n",
    "    cv=None,\n",
    "    n_jobs=None,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator instance\n",
    "        An estimator instance implementing `fit` and `predict` methods which\n",
    "        will be cloned for each validation.\n",
    "\n",
    "    title : str\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Training vector, where ``n_samples`` is the number of samples and\n",
    "        ``n_features`` is the number of features.\n",
    "\n",
    "    y : array-like of shape (n_samples) or (n_samples, n_features)\n",
    "        Target relative to ``X`` for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array-like of shape (3,), default=None\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple of shape (2,), default=None\n",
    "        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, default=None\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, default=None\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like of shape (n_ticks,)\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the ``dtype`` is float, it is regarded\n",
    "        as a fraction of the maximum size of the training set (that is\n",
    "        determined by the selected validation method), i.e. it has to be within\n",
    "        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n",
    "        sets. Note that for classification the number of samples usually have\n",
    "        to be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "        estimator,\n",
    "        X,\n",
    "        y,\n",
    "        groups=groups,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        train_sizes=train_sizes,\n",
    "        scoring=scoring,\n",
    "        return_times=True,\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "    )\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, \"o-\")\n",
    "    axes[1].fill_between(\n",
    "        train_sizes,\n",
    "        fit_times_mean - fit_times_std,\n",
    "        fit_times_mean + fit_times_std,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    fit_time_argsort = fit_times_mean.argsort()\n",
    "    fit_time_sorted = fit_times_mean[fit_time_argsort]\n",
    "    test_scores_mean_sorted = test_scores_mean[fit_time_argsort]\n",
    "    test_scores_std_sorted = test_scores_std[fit_time_argsort]\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_time_sorted, test_scores_mean_sorted, \"o-\")\n",
    "    axes[2].fill_between(\n",
    "        fit_time_sorted,\n",
    "        test_scores_mean_sorted - test_scores_std_sorted,\n",
    "        test_scores_mean_sorted + test_scores_std_sorted,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51115173",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RepeatedStratifiedGroupKFold():\n",
    "    def __init__(self, n_splits=5, n_repeats=10, random_state=None):\n",
    "        self.random_state = random_state\n",
    "        self.n_repeats = n_repeats\n",
    "        self.n_splits = n_splits\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        tries = 0\n",
    "        for idx in range(self.n_repeats):\n",
    "            invalid = True\n",
    "            # Skip the cv if any of its folds have only one class in the train/test split (causes errors in AUC calculation)\n",
    "            while (invalid):\n",
    "                cv = StratifiedGroupKFold(n_splits=self.n_splits, shuffle = True, random_state=self.random_state + tries)\n",
    "                invalid = False\n",
    "                tries = tries + 1\n",
    "                for train_index, test_index in cv.split(X, y, groups):\n",
    "                    trainPosRatio = y.iloc[train_index].mean()\n",
    "                    testPosRatio = y.iloc[test_index].mean()\n",
    "                    # test for if train fold and test fold only have one class\n",
    "                    if trainPosRatio == 0 or trainPosRatio == 1 or testPosRatio == 0 or testPosRatio == 1:\n",
    "                        invalid = True\n",
    "                        break\n",
    "                    \n",
    "            for train_index, test_index in cv.split(X, y, groups):\n",
    "                yield train_index, test_index\n",
    "\n",
    "    def get_n_splits(self, X, y, groups=None):\n",
    "        cv = StratifiedGroupKFold(n_splits=self.n_splits, shuffle = True, random_state=self.random_state)\n",
    "        return cv.get_n_splits(X, y, groups) * self.n_repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d548b6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Example RepeatedStratifiedGroupKFold split\n",
    "cv = RepeatedStratifiedGroupKFold(n_splits=10, n_repeats=10, random_state=5)\n",
    "print(\"ORIGINAL POSITIVE RATIO:\", y.mean())\n",
    "train_positive = []\n",
    "test_positive = []\n",
    "for fold, (train_idxs, test_idxs) in enumerate(cv.split(X, y, allSubjects)):\n",
    "    print(\"Fold :\", fold)\n",
    "    train_positive.append(y.iloc[train_idxs].mean())\n",
    "    test_positive.append(y.iloc[test_idxs].mean())\n",
    "    print(\"TRAIN POSITIVE RATIO:\", y.iloc[train_idxs].mean())\n",
    "    print(\"TEST POSITIVE RATIO :\", y.iloc[test_idxs].mean())\n",
    "    print(\"TRAIN GROUPS        :\", [allSubjects[i] for i in train_idxs])\n",
    "    print(\"TEST GROUPS         :\", [allSubjects[i] for i in test_idxs])\n",
    "print(\"ALL TRAIN POSITIVE RATIO:\", np.mean(train_positive))\n",
    "print(\"STD TRAIN POSITIVE RATIO:\", np.std(train_positive))\n",
    "print(\"ALL TEST POSITIVE RATIO :\", np.mean(test_positive))\n",
    "print(\"STD TEST POSITIVE RATIO:\", np.std(test_positive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68d5ffc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# global constants\n",
    "\n",
    "N_SPLITS = 10\n",
    "N_REPEATS = 10\n",
    "RD = 5\n",
    "K=10\n",
    "\n",
    "cv = RepeatedStratifiedGroupKFold(n_splits=N_SPLITS, n_repeats=N_REPEATS, random_state=RD)\n",
    "\n",
    "specificity = make_scorer(recall_score, pos_label=0)\n",
    "sensitivity = make_scorer(recall_score, pos_label=1)\n",
    "\n",
    "scoring = {'specificity': specificity,\n",
    "           'sensitivity': sensitivity,\n",
    "           'roc_auc': 'roc_auc'\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d8b1f6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ML Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894965af",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Feature Selection with Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f0e40f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Lasso(X_train): \n",
    "    # Define baseline lasso model\n",
    "    lasso_grid_pipe = Pipeline([\n",
    "        ('scalar', StandardScaler()),\n",
    "        ('clf', LogisticRegression(penalty = 'l1', solver='liblinear', max_iter = 10000))])\n",
    "\n",
    "    # Run grid search on hyperparameters\n",
    "    parameters = {'clf__C': [0.001, 0.01, 0.1, 0.5, 1.0, 10, 100, 1000],\n",
    "                  'clf__tol': [0.00001, 0.0001, 0.0005, 0.001,  0.005, 0.01] }\n",
    "\n",
    "    lasso_grid = GridSearchCV(lasso_grid_pipe, parameters, cv=cv, scoring = 'roc_auc')\n",
    "    lasso_grid.fit(X_train, y_train, groups=allSubjects)\n",
    "\n",
    "    # Get best model and run cross validation using all data\n",
    "    print('Best estimator: ', lasso_grid.best_estimator_)\n",
    "    lasso_pipe = lasso_grid.best_estimator_\n",
    "\n",
    "    scores = cross_validate(lasso_pipe, X_train, y_train, groups=allSubjects, cv=cv, scoring=scoring, n_jobs=-1, return_train_score=True)\n",
    "    print('Cross-Validation Evaluation Scores')\n",
    "    print('Train AUC:', 'mean-',np.mean(scores['train_roc_auc']), 'std dev-', np.std(scores['train_roc_auc']))\n",
    "    print('Test AUC:', 'mean-',np.mean(scores['test_roc_auc']), 'std dev-', np.std(scores['test_roc_auc']))\n",
    "    print('Sensitivity:', 'mean-',np.mean(scores['test_sensitivity']), 'std dev-', np.std(scores['test_sensitivity']))\n",
    "    print('Specificity:', 'mean-',np.mean(scores['test_specificity']), 'std dev-', np.std(scores['test_specificity']))\n",
    "    return lasso_grid.best_params_,lasso_grid.best_estimator_,np.mean(scores['test_roc_auc'])\n",
    "\n",
    "# Getting best lasso model\n",
    "lasso_best_param, lasso_best_model, lasso_scores = Lasso(X_train)\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "title = \"Learning Curves (Lasso)\"\n",
    "plot_learning_curve(\n",
    "    lasso_best_model, title, X_train, y_train, groups=allSubjects, scoring='roc_auc', axes=axes, ylim=(0.3, 1.01), cv=cv, n_jobs=4\n",
    ")\n",
    "print('Best parameters',lasso_best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3d5b1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# getting abs value of model coefficients for importance scores\n",
    "coefficients = lasso_best_model.named_steps['clf'].coef_\n",
    "importance = np.abs(coefficients).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1296ba6c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def FeatureSelection(Model):\n",
    "    # sort all the positive importance values to obtain different thresholds to filter the importance scores\n",
    "    posImportance = sorted([i for i in importance if i > 0])\n",
    "    importance_thresholds_index = np.round(np.linspace(0, len(posImportance) - 1, 10))\n",
    "    \n",
    "    highest_score = 0\n",
    "    # for each threshold, filter features lower than that threshold\n",
    "    for th_idx in importance_thresholds_index:\n",
    "        th = posImportance[int(th_idx)]\n",
    "        selected = np.array(feature_names)[importance >= th]\n",
    "        print(len(selected))\n",
    "        if (len(selected) > 30 or len(selected) < 3): # too many/too little features\n",
    "            continue\n",
    "            \n",
    "        # Train model on selected features\n",
    "        X_train = X[selected]\n",
    "        model_param, model, model_score = Model(X_train)\n",
    "        \n",
    "        # Update highest score and best threshold\n",
    "        if model_score > highest_score:\n",
    "            highest_score = model_score\n",
    "            best_threshold = th\n",
    "            best_model = model\n",
    "    return (highest_score, best_threshold, best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33eea7e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Running Lasso, Ridge, and LinearSVC with Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed530ee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Lasso Model\n",
    "(best_lasso_score, best_threshold, best_lasso_model) = FeatureSelection(Lasso)\n",
    "selected = np.array(feature_names)[importance >= best_threshold]\n",
    "X_train = X[selected]\n",
    "\n",
    "print('The threshold {} gives the highest AUC score {} with model\\n {}'.format(best_threshold, best_lasso_score, best_lasso_model))\n",
    "print('This threshold gives {} features which are {}'.format(len(selected), selected))\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "title = \"Learning Curves (Lasso)\"\n",
    "plot_learning_curve(\n",
    "    best_lasso_model, title, X_train, y_train, groups=allSubjects, scoring='roc_auc', axes=axes, ylim=(0.3, 1.01), cv=cv, n_jobs=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9437ff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def Ridge(X_train):\n",
    "    # Define baseline ridge model\n",
    "    ridge_grid_pipe = Pipeline([\n",
    "        ('scalar', StandardScaler()),\n",
    "        ('clf', LogisticRegression(penalty = 'l2'))])\n",
    "    \n",
    "    # Run grid search on hyperparameters\n",
    "    parameters = {'clf__C':[0.001, 0.01, .1, .25, .5, .75, 1.0, 5, 10, 50, 100],\n",
    "                  'clf__tol': [0.00001, 0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
    "                 }\n",
    "        \n",
    "    grid_ridge_reg = GridSearchCV(ridge_grid_pipe, parameters, cv=cv, scoring = 'roc_auc')\n",
    "    grid_ridge_reg.fit(X_train, y_train, groups=allSubjects)\n",
    "\n",
    "    # Get best model and run cross validation using all data\n",
    "    print('Best estimator: ', grid_ridge_reg.best_estimator_)\n",
    "    ridge_pipe = grid_ridge_reg.best_estimator_\n",
    "\n",
    "    scores = cross_validate(ridge_pipe, X_train, y_train, groups=allSubjects, cv=cv, scoring = scoring, n_jobs=-1, return_train_score=True) \n",
    "    print('Cross-Validation Evaluation Scores')\n",
    "    print('Train AUC:', 'mean-',np.mean(scores['train_roc_auc']), 'std dev-', np.std(scores['train_roc_auc']))\n",
    "    print('AUC:', 'mean-',np.mean(scores['test_roc_auc']), 'std dev-', np.std(scores['test_roc_auc']))\n",
    "    print('Sensitivity:', 'mean-',np.mean(scores['test_sensitivity']), 'std dev-', np.std(scores['test_sensitivity']))\n",
    "    print('Specificity:', 'mean-',np.mean(scores['test_specificity']), 'std dev-', np.std(scores['test_specificity']))\n",
    "    return grid_ridge_reg.best_params_, grid_ridge_reg.best_estimator_, np.mean(scores['test_roc_auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f2f791",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(best_ridge_score, best_threshold, best_ridge_model) = FeatureSelection(Ridge)\n",
    "selected = np.array(feature_names)[importance >= best_threshold]\n",
    "X_train = X[selected]\n",
    "\n",
    "print('The threshold {} gives the highest AUC score {} with model\\n {}'.format(best_threshold, best_ridge_score, best_ridge_model))\n",
    "print('This threshold gives {} features which are {}'.format(len(selected), selected))\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "title = \"Learning Curves (Ridge)\"\n",
    "plot_learning_curve(\n",
    "    best_ridge_model, title, X_train, y_train, groups=allSubjects, scoring='roc_auc', axes=axes, ylim=(0.3, 1.01), cv=cv, n_jobs=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce26f85",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def SVC_classifier(X_train):\n",
    "    # Define SVC ridge model\n",
    "    svc_grid_pipe = Pipeline([\n",
    "    ('scalar', StandardScaler()),\n",
    "    ('clf', LinearSVC(dual=False))])\n",
    " \n",
    "    # Run grid search on hyperparameters\n",
    "    param_grid = {'clf__C': [0.001, 0.01, 0.1, 0.5, 1.0, 10, 100, 1000, 1e4, 1e5],\n",
    "              'clf__tol': [0.00001, 0.0001, 0.0005, 0.001, 0.005, 0.01]}\n",
    "    \n",
    "    grid_svc = GridSearchCV(svc_grid_pipe, param_grid = param_grid, cv=cv, verbose = 0,n_jobs=-1,scoring = 'roc_auc') \n",
    "    grid_svc.fit(X_train, y_train, groups=allSubjects)\n",
    "\n",
    "    # Get best model and run cross validation using all data\n",
    "    print('Best estimator: ', grid_svc.best_estimator_)\n",
    "    svc_pipe = grid_svc.best_estimator_\n",
    "    \n",
    "    svc_scores = cross_validate(svc_pipe, X_train, y_train, groups=allSubjects, scoring=scoring, cv=cv, n_jobs=-1, return_train_score=True)\n",
    "    print('Cross-Validation Evaluation Scores')\n",
    "    print('Train AUC:', 'mean-',np.mean(svc_scores['train_roc_auc']), 'std dev-', np.std(svc_scores['train_roc_auc']))\n",
    "    print('AUC:', 'mean-',np.mean(svc_scores['test_roc_auc']), 'std dev-', np.std(svc_scores['test_roc_auc']))\n",
    "    print('Sensitivity:', 'mean-',np.mean(svc_scores['test_sensitivity']), 'std dev-', np.std(svc_scores['test_sensitivity']))\n",
    "    print('Specificity:', 'mean-',np.mean(svc_scores['test_specificity']), 'std dev-', np.std(svc_scores['test_specificity']))\n",
    "\n",
    "    return grid_svc.best_params_, grid_svc.best_estimator_, np.mean(svc_scores['test_roc_auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b4d25d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(best_svc_score, best_threshold, best_svc_model) = FeatureSelection(SVC_classifier)\n",
    "selected = np.array(feature_names)[importance >= best_threshold]\n",
    "X_train = X[selected]\n",
    "\n",
    "print('The threshold {} gives the highest AUC score {} with model\\n {}'.format(best_threshold, best_svc_score, best_svc_model))\n",
    "print('This threshold gives {} features which are {}'.format(len(selected), selected))\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "title = \"Learning Curves (LinearSVC)\"\n",
    "plot_learning_curve(\n",
    "    best_svc_model, title, X_train, y_train, groups=allSubjects, scoring='roc_auc', axes=axes, ylim=(0.3, 1.01), cv=cv, n_jobs=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c3c0f7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Cross analysis between Pre and Post3Mo SRS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cf82f4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Post3Mo Selected Features on Pre-SRS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e636ce08",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "post_selected = ['T1_original_shape_Sphericity', 'T1_original_shape_SurfaceVolumeRatio',\n",
    " 'T1_original_glcm_Imc1', 'T1_original_glcm_Idm', 'T1_original_glcm_Idmn',\n",
    " 'T1_original_glcm_InverseVariance',\n",
    " 'T1_original_gldm_DependenceNonUniformityNormalized',\n",
    " 'Vasc_original_firstorder_Maximum', 'Vasc_original_glcm_ClusterProminence',\n",
    " 'Vasc_original_glcm_Imc2',\n",
    " 'Vasc_original_glrlm_LongRunLowGrayLevelEmphasis',\n",
    " 'Vasc_original_glszm_GrayLevelNonUniformityNormalized',\n",
    " 'Vasc_original_glszm_GrayLevelVariance',\n",
    " 'Vasc_original_glszm_SmallAreaLowGrayLevelEmphasis',\n",
    " 'Vasc_original_gldm_DependenceNonUniformityNormalized',\n",
    " 'Vasc_original_gldm_SmallDependenceLowGrayLevelEmphasis',\n",
    " 'Vasc_original_ngtdm_Complexity', 'T2_original_firstorder_90Percentile',\n",
    " 'T2_original_firstorder_InterquartileRange',\n",
    " 'T2_original_firstorder_Kurtosis', 'T2_original_firstorder_Maximum',\n",
    " 'T2_original_firstorder_Skewness', 'T2_original_firstorder_Variance',\n",
    " 'T2_original_glcm_ClusterProminence', 'T2_original_glcm_JointEnergy',\n",
    " 'T2_original_glcm_Idm', 'T2_original_glszm_SmallAreaLowGrayLevelEmphasis',\n",
    " 'T2_original_gldm_SmallDependenceLowGrayLevelEmphasis',\n",
    " 'T2_original_ngtdm_Strength']\n",
    "X_sel = X[post_selected]\n",
    "# svc_best_params, svc_best_model, svc_score = SVC_classifier(X_train)\n",
    "# fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "\n",
    "# title = \"Learning Curves (LinearSVC on Post Features)\"\n",
    "# plot_learning_curve(\n",
    "#     svc_best_model, title, X_train, y_train, groups=allSubjects, scoring='roc_auc', axes=axes, ylim=(0.3, 1.01), cv=cv, n_jobs=4\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f951a24b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "svc_best_param, svc_best_model, svc_scores = SVC_classifier(X_sel)\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "\n",
    "title = \"Learning Curves (LinearSVC)\"\n",
    "plot_learning_curve(\n",
    "    svc_best_model, title, X_sel, y_train, groups=allSubjects, scoring='roc_auc', axes=axes, ylim=(0.3, 1.01), cv=cv, n_jobs=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13299c7c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Pre-SRS Model on Post3Mo SRS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a47c10",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialize the file paths to hold all the images separated by sequence and SRS Date\n",
    "root = '/Users/cxl037/PycharmProjects/pythonProject1/Example_Data/'\n",
    "Post3MoT1Bravo_path = os.path.join(root, 'Post3MoT1Bravo_Immuno')\n",
    "Post3MoT1Bravo_batchfile = os.path.join(Post3MoT1Bravo_path, 'radiomics_features.csv')\n",
    "Post3MoT1Vasc_path = os.path.join(root, 'Post3MoT1Vasc_Immuno')\n",
    "Post3MoT1Vasc_batchfile = os.path.join(Post3MoT1Vasc_path, 'radiomics_features.csv')\n",
    "Post3MoT2Flair_path = os.path.join(root, 'Post3MoT2Flair_Immuno')\n",
    "Post3MoT2Flair_batchfile = os.path.join(Post3MoT2Flair_path, 'radiomics_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1c8de7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get patientID to merge radiomic_features.csv and SRS_immune_list.xlsx\n",
    "extractedFeatures_post = pd.read_csv(Post3MoT1Bravo_batchfile)\n",
    "extractedFeatures_post = extractedFeatures_post.add_prefix('T1_')\n",
    "extractedFeatures_post.rename(columns = {'T1_Mask': 'Mask'}, inplace = True)\n",
    "extractedFeatures_post['PatientID'] = extractedFeatures_post.apply(lambda row: row['Mask'][:7], axis=1)\n",
    "extractedFeatures_post['PatientID'] = extractedFeatures_post['PatientID'].astype(str)\n",
    "\n",
    "# Get patientID to merge radiomic_features.csv and SRS_immune_list.xlsx\n",
    "extractedFeatures2_post = pd.read_csv(Post3MoT1Vasc_batchfile)\n",
    "extractedFeatures2_post = extractedFeatures2_post.add_prefix('Vasc_')\n",
    "extractedFeatures2_post.rename(columns = {'Vasc_Mask': 'Mask'}, inplace = True)\n",
    "\n",
    "# Get patientID to merge radiomic_features.csv and SRS_immune_list.xlsx\n",
    "extractedFeatures3_post = pd.read_csv(Post3MoT2Flair_batchfile)\n",
    "extractedFeatures3_post = extractedFeatures3_post.add_prefix('T2_')\n",
    "extractedFeatures3_post.rename(columns = {'T2_Mask': 'Mask'}, inplace = True)\n",
    "\n",
    "allFeatures = pd.merge(extractedFeatures_post, patientDetails, on='PatientID', how = 'left' )\n",
    "allFeatures = pd.merge(allFeatures, extractedFeatures2_post, on='Mask', how = 'left' )\n",
    "allFeatures = pd.merge(allFeatures, extractedFeatures3_post, on='Mask', how = 'left' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8794b5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "remove_cols = [feature for feature in allFeatures.columns \n",
    "               if not (feature.startswith(\"T2_original\") or feature.startswith(\"T1_original\") or feature.startswith(\"Vasc_original\") \n",
    "               or feature == 'Immunotherapy_prior_3_months' or feature == 'PatientID')]\n",
    "filteredFeatures = allFeatures.drop(remove_cols, axis = 1)\n",
    "filteredFeatures = filteredFeatures.dropna() # remove rows with NA values, only preserving those with T1 and T2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0320dfc1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_post = filteredFeatures.drop(['Immunotherapy_prior_3_months', 'PatientID'], axis=1)\n",
    "y_post = filteredFeatures['Immunotherapy_prior_3_months']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57911aab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pre_selected = ['T1_original_shape_Sphericity', 'T1_original_glcm_Correlation',\n",
    " 'T1_original_glcm_Imc1',\n",
    " 'T1_original_glszm_SizeZoneNonUniformityNormalized',\n",
    " 'T1_original_gldm_DependenceVariance',\n",
    " 'T1_original_gldm_LargeDependenceHighGrayLevelEmphasis',\n",
    " 'Vasc_original_glcm_ClusterShade', 'Vasc_original_glcm_Idmn',\n",
    " 'Vasc_original_glszm_GrayLevelNonUniformityNormalized',\n",
    " 'Vasc_original_gldm_DependenceNonUniformityNormalized',\n",
    " 'T2_original_gldm_LargeDependenceHighGrayLevelEmphasis',\n",
    " 'T2_original_gldm_SmallDependenceLowGrayLevelEmphasis']\n",
    "X_sel_post = X_post[pre_selected]\n",
    "X_sel_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e938f2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "roc_auc_score(y_post, best_svc_model.predict(X_sel_post))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd05fb3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Neural Network Model\n",
    "\n",
    "Hyperparameter tuning with Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1bbebc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adadelta, Adam\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras.metrics import AUC\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping,CSVLogger\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf1b08",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X[pre_selected]\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb918e1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_scaled = pd.DataFrame(sc.fit_transform(X_train))\n",
    "y_train = y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2619f8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "display(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49da433",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_dim = len(x_train.columns)\n",
    "output_dim = 1\n",
    "print (\"input\", input_dim, \"output\", output_dim)\n",
    "\n",
    "#Hyper-Parameters\n",
    "layer1 = 1\n",
    "layer2 = 2\n",
    "layer3 = 3\n",
    "dropout1 = 0.3\n",
    "dropout2 = .5\n",
    "dropout3 = .45801654819487025\n",
    "batch_size = 16\n",
    "optimizer = Adam(\n",
    "    learning_rate=0.008\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a08e9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_clf(layer1, dropout1, learning_rate):\n",
    "    # creating the layers of the NN\n",
    "    input_img = Input(shape=(input_dim,))\n",
    "    deep = Dense(layer1, activation='relu')(input_img)\n",
    "    deep = Dropout(dropout1)(deep)\n",
    "    outlayer = Dense(output_dim, activation='sigmoid')(deep)\n",
    "    model = Model(input_img, outlayer)\n",
    "    model.compile(loss=['binary_crossentropy'], metrics=[AUC()],#, 'sparse_categorical_accuracy'\n",
    "                        optimizer= Adam(learning_rate=learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a8003",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model=KerasClassifier(model=build_clf, layer1=1, dropout1=0.1, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e814d4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(model.get_params().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33366ac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params={'batch_size': [1, 2, 4, 8, 16, 32, 64], \n",
    "        'dropout1': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        'layer1': [1, 2, 3, 4, 5],\n",
    "        'learning_rate': [1e-4, 1e-3, 0.001, 0.01]\n",
    "        }\n",
    "gs = GridSearchCV(estimator=model, param_grid=params, cv=cv, verbose = 3)\n",
    "# now fit the dataset to the GridSearchCV object. \n",
    "gs = gs.fit(X_scaled, y_train, groups=allSubjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b69455",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mlp_best_params = gs.best_params_\n",
    "mlp_best_score = gs.best_score_\n",
    "mlp_best_model = gs.best_estimator_\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "\n",
    "title = \"Learning Curves (MLP)\"\n",
    "plot_learning_curve(\n",
    "    mlp_best_model, title, X_sel, y_train, groups=allSubjects, scoring='roc_auc', axes=axes, ylim=(0.3, 1.01), cv=cv, n_jobs=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5944637c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#MLP\n",
    "input_img = Input(shape=(input_dim,))\n",
    "deep = Dense(layer1, activation='relu')(input_img)\n",
    "deep = Dropout(dropout1)(deep)\n",
    "# deep = Dense(layer2, activation='relu')(deep)\n",
    "# deep = Dropout(dropout2)(deep)\n",
    "#deep = Dense(layer3, activation='relu')(deep)\n",
    "#deep = Dropout(dropout3)(deep)\n",
    "outlayer = Dense(output_dim, activation='sigmoid')(deep)\n",
    "model = Model(input_img, outlayer)\n",
    "\n",
    "model.compile(loss=['binary_crossentropy'], metrics=[AUC()],#, 'sparse_categorical_accuracy'\n",
    "                        optimizer= optimizer)\n",
    "print(model.summary())\n",
    "print(model.metrics_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221dfe7e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=200)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=2000, #int(params['n_epochs']),\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    verbose=2,\n",
    "                    validation_data=(X_test, y_test),callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bc6259",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot loss during training\n",
    "plt.subplot(211)\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['val_loss'], label='valid')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.legend()\n",
    "# plot accuracy during training\n",
    "plt.subplot(212)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['val_auc_33'], label='valid')\n",
    "plt.plot(history.history['auc_33'], label='train')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5757cf60",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c104b17a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_post = filteredFeatures.drop(['Immunotherapy_prior_3_months', 'PatientID'], axis=1)\n",
    "y_post = filteredFeatures['Immunotherapy_prior_3_months']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd70548",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pre_selected = ['T1_original_shape_Sphericity', 'T1_original_glcm_Correlation',\n",
    " 'T1_original_glcm_Imc1',\n",
    " 'T1_original_glszm_SizeZoneNonUniformityNormalized',\n",
    " 'T1_original_gldm_DependenceVariance',\n",
    " 'T1_original_gldm_LargeDependenceHighGrayLevelEmphasis',\n",
    " 'Vasc_original_glcm_ClusterShade', 'Vasc_original_glcm_Idmn',\n",
    " 'Vasc_original_glszm_GrayLevelNonUniformityNormalized',\n",
    " 'Vasc_original_gldm_DependenceNonUniformityNormalized',\n",
    " 'T2_original_gldm_LargeDependenceHighGrayLevelEmphasis',\n",
    " 'T2_original_gldm_SmallDependenceLowGrayLevelEmphasis']\n",
    "X_sel_post = X_post[pre_selected]\n",
    "X_sel_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca9b723",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "roc_auc_score(y_post, best_svc_model.predict(X_sel_post))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
