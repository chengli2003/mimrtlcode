{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a1cb6d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import glob\n",
    "import shutil\n",
    "import seaborn as sns\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aafbff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.feature_selection import RFE, SelectKBest, mutual_info_classif,f_classif\n",
    "from sklearn.model_selection import (KFold, train_test_split, cross_validate, GridSearchCV, RepeatedStratifiedKFold,\n",
    "                                     cross_val_score, GroupKFold, StratifiedGroupKFold, StratifiedKFold)\n",
    "from sklearn.metrics import roc_auc_score, recall_score, make_scorer, f1_score, confusion_matrix, roc_curve, accuracy_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis                              \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from feature_engine.selection import RecursiveFeatureAddition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff40508",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40f10fa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialize the file paths to hold all the images separated by sequence and SRS Date\n",
    "root = '/Users/cxl037/PycharmProjects/pythonProject1/Example_Data/'\n",
    "root_ML = '/Users/cxl037/PycharmProjects/DeepLearning'\n",
    "model_path = os.path.join(root_ML, 'Models')\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)\n",
    "PreT1Bravo_path = os.path.join(root, 'PreT1Bravo_Immuno')\n",
    "PreT1Bravo_batchfile = os.path.join(PreT1Bravo_path, 'radiomics_features3.csv')\n",
    "PreT1Vasc_path = os.path.join(root, 'PreT1Vasc_Immuno')\n",
    "PreT1Vasc_batchfile = os.path.join(PreT1Vasc_path, 'radiomics_features.csv')\n",
    "PreT2Flair_path = os.path.join(root, 'PreT2Flair_Immuno')\n",
    "PreT2Flair_batchfile = os.path.join(PreT2Flair_path, 'radiomics_features.csv')\n",
    "Post3MoT1Bravo_path = os.path.join(root, 'Post3MoT1Bravo_Immuno')\n",
    "Post3MoT1Bravo_batchfile = os.path.join(Post3MoT1Bravo_path, 'radiomics_features.csv')\n",
    "Post3MoT1Vasc_path = os.path.join(root, 'Post3MoT1Vasc_Immuno')\n",
    "Post3MoT1Vasc_batchfile = os.path.join(Post3MoT1Vasc_path, 'radiomics_features.csv')\n",
    "Post3MoT2Flair_path = os.path.join(root, 'Post3MoT2Flair_Immuno')\n",
    "Post3MoT2Flair_batchfile = os.path.join(Post3MoT2Flair_path, 'radiomics_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f30831d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read T1Bravo data and add T1_ prefix to all the feature columns\n",
    "extractedFeatures = pd.read_csv(PreT1Bravo_batchfile)\n",
    "extractedFeatures = extractedFeatures.add_prefix('T1_')\n",
    "extractedFeatures.rename(columns = {'T1_Mask': 'Mask'}, inplace = True)\n",
    "# Get patientID for future merging\n",
    "extractedFeatures['PatientID'] = extractedFeatures.apply(lambda row: row['Mask'][:7], axis=1)\n",
    "extractedFeatures['PatientID'] = extractedFeatures['PatientID'].astype(str)\n",
    "extractedFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb6f9c4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read T1Vasc data and add Vasc_ prefix to all the feature columns\n",
    "extractedFeatures2 = pd.read_csv(PreT1Vasc_batchfile)\n",
    "extractedFeatures2 = extractedFeatures2.add_prefix('Vasc_')\n",
    "extractedFeatures2.rename(columns = {'Vasc_Mask': 'Mask'}, inplace = True)\n",
    "extractedFeatures2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62c09a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read T2Flair data and add T2_ prefix to all the feature columns\n",
    "extractedFeatures3 = pd.read_csv(PreT2Flair_batchfile)\n",
    "extractedFeatures3 = extractedFeatures3.add_prefix('T2_')\n",
    "extractedFeatures3.rename(columns = {'T2_Mask': 'Mask'}, inplace = True)\n",
    "extractedFeatures3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2891df57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all the pre sequences together\n",
    "allPreFeatures = pd.merge(extractedFeatures, extractedFeatures2, on='Mask', how = 'left' )\n",
    "allPreFeatures = pd.merge(allPreFeatures, extractedFeatures3, on='Mask', how = 'left' )\n",
    "# Define label as 0 for pre-SRS features\n",
    "allPreFeatures['SRS Status'] = 0\n",
    "allPreFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ae5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read T1Bravo data and add T1_ prefix to all the feature columns\n",
    "extractedFeatures4 = pd.read_csv(Post3MoT1Bravo_batchfile)\n",
    "extractedFeatures4 = extractedFeatures4.add_prefix('T1_')\n",
    "extractedFeatures4.rename(columns = {'T1_Mask': 'Mask'}, inplace = True)\n",
    "# Get patientID for future merging\n",
    "extractedFeatures4['PatientID'] = extractedFeatures4.apply(lambda row: row['Mask'][:7], axis=1)\n",
    "extractedFeatures4['PatientID'] = extractedFeatures4['PatientID'].astype(str)\n",
    "extractedFeatures4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e64d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read T1Vasc data and add Vasc_ prefix to all the feature columns\n",
    "extractedFeatures5 = pd.read_csv(Post3MoT1Vasc_batchfile)\n",
    "extractedFeatures5 = extractedFeatures5.add_prefix('Vasc_')\n",
    "extractedFeatures5.rename(columns = {'Vasc_Mask': 'Mask'}, inplace = True)\n",
    "extractedFeatures5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb9b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read T2Flair data and add T2_ prefix to all the feature columns\n",
    "extractedFeatures6 = pd.read_csv(Post3MoT2Flair_batchfile)\n",
    "extractedFeatures6 = extractedFeatures6.add_prefix('T2_')\n",
    "extractedFeatures6.rename(columns = {'T2_Mask': 'Mask'}, inplace = True)\n",
    "extractedFeatures6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b66420",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Merge all the post sequences together\n",
    "allPostFeatures = pd.merge(extractedFeatures4, extractedFeatures5, on='Mask', how = 'left' )\n",
    "allPostFeatures = pd.merge(allPostFeatures, extractedFeatures6, on='Mask', how = 'left' )\n",
    "# Define label as 1 for post-SRS features\n",
    "allPostFeatures['SRS Status'] = 1\n",
    "allPostFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08becc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine into one dataframe\n",
    "allFeatures = pd.concat([allPreFeatures, allPostFeatures], ignore_index=True)\n",
    "allFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f7efc3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Drop all diagnostic features\n",
    "remove_cols = [feature for feature in allFeatures.columns \n",
    "               if not (feature.startswith(\"T2_original\") or feature.startswith(\"T1_original\") or feature.startswith(\"Vasc_original\") \n",
    "               or feature == 'SRS Status' or feature == 'PatientID')]\n",
    "filteredFeatures = allFeatures.drop(remove_cols, axis = 1)\n",
    "# remove rows with NA values, only preserving those rows which have information for all sequences\n",
    "filteredFeatures = filteredFeatures.dropna()\n",
    "# Get all the subject ids (useful for RepeatedStratifiedGroupKFold)\n",
    "allSubjects = list(filteredFeatures.loc[:, 'PatientID'])\n",
    "print(len(set(allSubjects)))\n",
    "print(len(allSubjects))\n",
    "filteredFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12289720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data and labels\n",
    "X = filteredFeatures.drop(['SRS Status', 'PatientID'], axis=1)\n",
    "y = filteredFeatures['SRS Status']\n",
    "feature_names = list(X.columns)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b169ed79",
   "metadata": {},
   "source": [
    "## Defining Functions and Variables for ML Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d5b11",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RepeatedStratifiedGroupKFold():\n",
    "    def __init__(self, n_splits=5, n_repeats=10, random_state=None):\n",
    "        self.random_state = random_state\n",
    "        self.n_repeats = n_repeats\n",
    "        self.n_splits = n_splits\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        tries = 0\n",
    "        X = pd.DataFrame(X)\n",
    "        y = pd.DataFrame(y)\n",
    "        for idx in range(self.n_repeats):\n",
    "            invalid = True\n",
    "            # Skip the cv if any of its folds have only one class in the train/test split (causes errors in AUC calculation)\n",
    "            while (invalid):\n",
    "                cv = StratifiedGroupKFold(n_splits=self.n_splits, shuffle = True, random_state=self.random_state + tries)\n",
    "                invalid = False\n",
    "                tries = tries + 1\n",
    "                for train_index, test_index in cv.split(X, y, groups):\n",
    "                    trainPosRatio = y.iloc[train_index].mean().item()\n",
    "                    testPosRatio = y.iloc[test_index].mean().item()\n",
    "                    # test for if train fold and test fold only have one class\n",
    "                    if trainPosRatio == 0 or trainPosRatio == 1 or testPosRatio == 0 or testPosRatio == 1:\n",
    "                        invalid = True\n",
    "                        break\n",
    "                    \n",
    "            for train_index, test_index in cv.split(X, y, groups):\n",
    "                yield train_index, test_index\n",
    "\n",
    "    def get_n_splits(self, X, y, groups=None):\n",
    "        cv = StratifiedGroupKFold(n_splits=self.n_splits, shuffle = True, random_state=self.random_state)\n",
    "        return cv.get_n_splits(X, y, groups) * self.n_repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10cdc2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Example RepeatedStratifiedGroupKFold split\n",
    "print(\"ORIGINAL POSITIVE RATIO:\", y.mean())\n",
    "train_positive = []\n",
    "test_positive = []\n",
    "for fold, (train_idxs, test_idxs) in enumerate(outercv.split(X, y, allSubjects)):\n",
    "    print(\"Repeat :\", fold // N_REPEATS_O + 1)\n",
    "    print(\"Fold :\", fold % N_REPEATS_O + 1)\n",
    "    print(\"train length:\", len(train_idxs))\n",
    "    print(\"test length:\", len(test_idxs))\n",
    "    train_positive.append(y.iloc[train_idxs].mean())\n",
    "    test_positive.append(y.iloc[test_idxs].mean())\n",
    "    print(\"TRAIN POSITIVE RATIO:\", y.iloc[train_idxs].mean())\n",
    "    print(\"TEST POSITIVE RATIO :\", y.iloc[test_idxs].mean())\n",
    "    print(\"TRAIN GROUPS        :\", [allSubjects[i] for i in train_idxs])\n",
    "    print(\"TEST GROUPS         :\", [allSubjects[i] for i in test_idxs])\n",
    "print(\"ALL TRAIN POSITIVE RATIO:\", np.mean(train_positive))\n",
    "print(\"STD TRAIN POSITIVE RATIO:\", np.std(train_positive))\n",
    "print(\"ALL TEST POSITIVE RATIO :\", np.mean(test_positive))\n",
    "print(\"STD TEST POSITIVE RATIO:\", np.std(test_positive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef401e89",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# global constants\n",
    "\n",
    "N_SPLITS_I = 5\n",
    "N_REPEATS_I = 5\n",
    "N_SPLITS_O = 5\n",
    "N_REPEATS_O = 5\n",
    "\n",
    "innercv = RepeatedStratifiedGroupKFold(n_splits=N_SPLITS_I, n_repeats=N_REPEATS_I, random_state=30)\n",
    "outercv = RepeatedStratifiedGroupKFold(n_splits=N_SPLITS_O, n_repeats=N_REPEATS_O, random_state=5)\n",
    "\n",
    "specificity = make_scorer(recall_score, pos_label=0)\n",
    "sensitivity = make_scorer(recall_score, pos_label=1)\n",
    "\n",
    "scoring = {'specificity': specificity,\n",
    "           'sensitivity': sensitivity,\n",
    "           'roc_auc': 'roc_auc'\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39e71d5",
   "metadata": {},
   "source": [
    "## ML Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8444d4e9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def Lasso(X_train, y_train): \n",
    "    lasso_grid_pipe = Pipeline([\n",
    "        ('scalar', StandardScaler()),\n",
    "        ('clf', LogisticRegression(penalty='l1', solver='liblinear', max_iter=20000))])\n",
    "    \n",
    "    lasso_grid_pipe.fit(X_train, y_train)\n",
    "    return lasso_grid_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe6d228",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def Ridge(X_train, y_train, train_subjects):\n",
    "    ridge_grid_pipe = Pipeline([\n",
    "        ('scalar', StandardScaler()),\n",
    "        ('clf', LogisticRegression(penalty = 'l2', solver='lbfgs',class_weight='balanced', max_iter=10000))])\n",
    "    \n",
    "    parameters = {'clf__C':[0.0001, 0.001, 0.01, .1, .5, 1.0, 5, 10, 50, 100],\n",
    "                  'clf__tol': [0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
    "                 }\n",
    "    \n",
    "    grid_ridge_reg = GridSearchCV(ridge_grid_pipe, parameters, cv=innercv, scoring = 'roc_auc', verbose = 0)\n",
    "    grid_ridge_reg.fit(X_train, y_train, groups=train_subjects)\n",
    "\n",
    "    print('Best estimator: ', grid_ridge_reg.best_estimator_)\n",
    "\n",
    "    ridge_pipe = grid_ridge_reg.best_estimator_\n",
    "\n",
    "    scores = cross_validate(ridge_pipe, X_train, y_train, groups=train_subjects, cv=innercv, scoring = scoring, n_jobs=-1, return_train_score=True)\n",
    "        \n",
    "    print('Cross-Validation Evaluation Scores')\n",
    "\n",
    "    print('Train AUC:', 'mean-',np.mean(scores['train_roc_auc']), 'std dev-', np.std(scores['train_roc_auc']))\n",
    "    print('Cross Val AUC:', 'mean-',np.mean(scores['test_roc_auc']), 'std dev-', np.std(scores['test_roc_auc']))\n",
    "    print('Sensitivity:', 'mean-',np.mean(scores['test_sensitivity']), 'std dev-', np.std(scores['test_sensitivity']))\n",
    "    print('Specificity:', 'mean-',np.mean(scores['test_specificity']), 'std dev-', np.std(scores['test_specificity']))\n",
    "\n",
    "    return grid_ridge_reg.best_params_, grid_ridge_reg.best_estimator_, scores['test_roc_auc'], scores['train_roc_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69c5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Elastic(X_train, y_train, train_subjects): \n",
    "    elastic_grid_pipe = Pipeline([\n",
    "        ('scalar', StandardScaler()),\n",
    "        ('clf', LogisticRegression(penalty = 'elasticnet', solver = 'saga', max_iter=10000))])\n",
    "\n",
    "    parameters = {'clf__C': [0.01, 0.1, 0.5, 1.0],\n",
    "                  'clf__l1_ratio': [.1, .5, .7, .9, .95, .99]}\n",
    "\n",
    "    elastic_grid = GridSearchCV(elastic_grid_pipe, parameters, cv=innercv, scoring = 'roc_auc', verbose=0, n_jobs=-1)\n",
    "    elastic_grid.fit(X_train, y_train, groups=train_subjects)\n",
    "\n",
    "    print('Best estimator: ', elastic_grid.best_estimator_)\n",
    "    elastic_pipe = elastic_grid.best_estimator_\n",
    "\n",
    "    scores = cross_validate(elastic_pipe, X_train, y_train, groups=train_subjects, cv=innercv, scoring=scoring, n_jobs=-1, return_train_score=True)\n",
    "    \n",
    "    print('Cross-Validation Evaluation Scores')\n",
    "\n",
    "    print('Train AUC:', 'mean-',np.mean(scores['train_roc_auc']), 'std dev-', np.std(scores['train_roc_auc']))\n",
    "    print('Test AUC:', 'mean-',np.mean(scores['test_roc_auc']), 'std dev-', np.std(scores['test_roc_auc']))\n",
    "    print('Sensitivity:', 'mean-',np.mean(scores['test_sensitivity']), 'std dev-', np.std(scores['test_sensitivity']))\n",
    "    print('Specificity:', 'xmean-',np.mean(scores['test_specificity']), 'std dev-', np.std(scores['test_specificity']))\n",
    "    return elastic_grid.best_params_, elastic_grid.best_estimator_, scores['test_roc_auc'], scores['train_roc_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dff68d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def SVC_linear(X_train, y_train, train_subjects):\n",
    "    svc_grid_pipe = Pipeline([\n",
    "    ('scalar', StandardScaler()),\n",
    "    ('clf', LinearSVC(dual=False, max_iter = 20000))])\n",
    " \n",
    "    param_grid = {'clf__C': [0.001, 0.01, 0.1, 0.5, 1.0, 10, 50],\n",
    "              'clf__tol': [0.0001, 0.0005, 0.001, 0.005, 0.01]}\n",
    "        \n",
    "    grid_svc = GridSearchCV(svc_grid_pipe, param_grid = param_grid, cv=innercv, verbose = 0,n_jobs=-1,scoring = 'roc_auc') \n",
    "    grid_svc.fit(X_train, y_train, groups=train_subjects)\n",
    "    \n",
    "    print('Best estimator: ', grid_svc.best_estimator_)\n",
    "\n",
    "    svc_pipe = grid_svc.best_estimator_\n",
    "    \n",
    "    svc_scores = cross_validate(svc_pipe, X_train, y_train, groups=train_subjects, scoring=scoring, cv=innercv, n_jobs=-1, return_train_score=True)\n",
    "    \n",
    "    print('Cross-Validation Evaluation Scores')\n",
    "\n",
    "    print('Train AUC:', 'mean-',np.mean(svc_scores['train_roc_auc']), 'std dev-', np.std(svc_scores['train_roc_auc']))\n",
    "    print('Cross Val AUC:', 'mean-',np.mean(svc_scores['test_roc_auc']), 'std dev-', np.std(svc_scores['test_roc_auc']))\n",
    "    print('Sensitivity:', 'mean-',np.mean(svc_scores['test_sensitivity']), 'std dev-', np.std(svc_scores['test_sensitivity']))\n",
    "    print('Specificity:', 'mean-',np.mean(svc_scores['test_specificity']), 'std dev-', np.std(svc_scores['test_specificity']))\n",
    "\n",
    "    return grid_svc.best_params_, grid_svc.best_estimator_, svc_scores['test_roc_auc'], svc_scores['train_roc_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5630349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVC_poly(X_train, y_train, train_subjects):\n",
    "    svc_grid_pipe = Pipeline([\n",
    "    ('scalar', StandardScaler()),\n",
    "    ('clf', SVC(kernel='poly', max_iter = 20000))])\n",
    " \n",
    "    param_grid = {'clf__C': [0.001, 0.01, 0.1, 0.5, 1.0, 10],\n",
    "                  'clf__gamma': [0.001, 0.01, 0.1, 1, 10]}\n",
    "    \n",
    "    grid_svc = GridSearchCV(svc_grid_pipe, param_grid = param_grid, cv=innercv, verbose = 0,n_jobs=-1,scoring = 'roc_auc') \n",
    "    grid_svc.fit(X_train, y_train, groups=train_subjects)\n",
    "    \n",
    "    print('Best estimator: ', grid_svc.best_estimator_)\n",
    "\n",
    "    svc_pipe = grid_svc.best_estimator_\n",
    "    \n",
    "    svc_scores = cross_validate(svc_pipe, X_train, y_train, groups=train_subjects, scoring=scoring, cv=innercv, n_jobs=-1, return_train_score=True)\n",
    "    \n",
    "    print('Cross-Validation Evaluation Scores')\n",
    "\n",
    "    print('Train AUC:', 'mean-',np.mean(svc_scores['train_roc_auc']), 'std dev-', np.std(svc_scores['train_roc_auc']))\n",
    "    print('Cross Val AUC:', 'mean-',np.mean(svc_scores['test_roc_auc']), 'std dev-', np.std(svc_scores['test_roc_auc']))\n",
    "    print('Sensitivity:', 'mean-',np.mean(svc_scores['test_sensitivity']), 'std dev-', np.std(svc_scores['test_sensitivity']))\n",
    "    print('Specificity:', 'mean-',np.mean(svc_scores['test_specificity']), 'std dev-', np.std(svc_scores['test_specificity']))\n",
    "\n",
    "    return grid_svc.best_params_, grid_svc.best_estimator_, svc_scores['test_roc_auc'], svc_scores['train_roc_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1699c3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVC_rbf(X_train, y_train, train_subjects):\n",
    "    svc_grid_pipe = Pipeline([\n",
    "    ('scalar', StandardScaler()),\n",
    "    ('clf', SVC(kernel='rbf', max_iter = 20000))])\n",
    " \n",
    "    param_grid = {'clf__C': [0.001, 0.01, 0.1, 0.5, 1.0, 10, 50],\n",
    "              'clf__gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10]}\n",
    "        \n",
    "    grid_svc = GridSearchCV(svc_grid_pipe, param_grid = param_grid, cv=innercv, verbose = 0,n_jobs=-1,scoring = 'roc_auc') \n",
    "    grid_svc.fit(X_train, y_train, groups=train_subjects)\n",
    "    \n",
    "    print('Best estimator: ', grid_svc.best_estimator_)\n",
    "\n",
    "    svc_pipe = grid_svc.best_estimator_\n",
    "    \n",
    "    svc_scores = cross_validate(svc_pipe, X_train, y_train, groups=train_subjects, scoring=scoring, cv=innercv, n_jobs=-1, return_train_score=True)\n",
    "    \n",
    "    print('Cross-Validation Evaluation Scores')\n",
    "\n",
    "    print('Train AUC:', 'mean-',np.mean(svc_scores['train_roc_auc']), 'std dev-', np.std(svc_scores['train_roc_auc']))\n",
    "    print('Cross Val AUC:', 'mean-',np.mean(svc_scores['test_roc_auc']), 'std dev-', np.std(svc_scores['test_roc_auc']))\n",
    "    print('Sensitivity:', 'mean-',np.mean(svc_scores['test_sensitivity']), 'std dev-', np.std(svc_scores['test_sensitivity']))\n",
    "    print('Specificity:', 'mean-',np.mean(svc_scores['test_specificity']), 'std dev-', np.std(svc_scores['test_specificity']))\n",
    "\n",
    "    return grid_svc.best_params_, grid_svc.best_estimator_, svc_scores['test_roc_auc'], svc_scores['train_roc_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcdc5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(X_train, y_train, train_subjects):\n",
    "    knn_grid_pipe = Pipeline([\n",
    "    ('scalar', StandardScaler()),\n",
    "    ('clf', KNeighborsClassifier())])\n",
    " \n",
    "    param_grid = {'clf__n_neighbors': [1, 2, 4, 8, 16, 32, 64],\n",
    "                  'clf__weights': ['uniform', 'distance']}\n",
    "        \n",
    "    grid_knn = GridSearchCV(knn_grid_pipe, param_grid = param_grid, cv=innercv, verbose = 0,n_jobs=-1,scoring = 'roc_auc') \n",
    "    grid_knn.fit(X_train, y_train, groups=train_subjects)\n",
    "    \n",
    "    print('Best estimator: ', grid_knn.best_estimator_)\n",
    "\n",
    "    knn_pipe = grid_knn.best_estimator_\n",
    "    \n",
    "    scores = cross_validate(knn_pipe, X_train, y_train, groups=train_subjects, scoring=scoring, cv=innercv, n_jobs=-1, return_train_score=True)\n",
    "    \n",
    "    print('Cross-Validation Evaluation Scores')\n",
    "\n",
    "    print('Train AUC:', 'mean-',np.mean(scores['train_roc_auc']), 'std dev-', np.std(scores['train_roc_auc']))\n",
    "    print('Cross Val AUC:', 'mean-',np.mean(scores['test_roc_auc']), 'std dev-', np.std(scores['test_roc_auc']))\n",
    "    print('Sensitivity:', 'mean-',np.mean(scores['test_sensitivity']), 'std dev-', np.std(scores['test_sensitivity']))\n",
    "    print('Specificity:', 'mean-',np.mean(scores['test_specificity']), 'std dev-', np.std(scores['test_specificity']))\n",
    "\n",
    "    return grid_knn.best_params_, grid_knn.best_estimator_, scores['test_roc_auc'], scores['train_roc_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d12d76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, y_train, train_subjects):\n",
    "    rf_grid_pipe = Pipeline([('clf', RandomForestClassifier())])\n",
    " \n",
    "    param_grid = {\n",
    "        'clf__n_estimators': [20,30, 50, 100,200,500,600,700],\n",
    "        'clf__max_features': ['sqrt', 'log2'],\n",
    "        'clf__max_depth': [1,3,5,10, 15, 20]}\n",
    "\n",
    "    grid_rfc = GridSearchCV(rf_grid_pipe, param_grid = param_grid, cv = innercv, scoring= 'roc_auc', verbose = 3)\n",
    "    grid_rfc.fit(X_train, y_train, groups=train_subjects)\n",
    "    print('Best estimator: ', grid_rfc.best_estimator_)\n",
    "\n",
    "    rfc_pipe = grid_rfc.best_estimator_\n",
    "\n",
    "    rfc_scores = cross_validate(rfc_pipe, X_train, y_train, groups=train_subjects, scoring = scoring, cv=innercv, n_jobs=-1)\n",
    "    \n",
    "    \n",
    "    print('Cross-Validation Evaluation Scores')\n",
    "\n",
    "    print('AUC:', 'mean-',np.mean(rfc_scores['test_roc_auc']), 'std dev-', np.std(rfc_scores['test_roc_auc']))\n",
    "    print('Sensitivity:', 'mean-',np.mean(rfc_scores['test_sensitivity']), 'std dev-', np.std(rfc_scores['test_sensitivity']))\n",
    "    print('Specificity:', 'mean-',np.mean(rfc_scores['test_specificity']), 'std dev-', np.std(rfc_scores['test_specificity']))\n",
    "    return grid_rfc.best_params_,grid_rfc.best_estimator_, rfc_scores['test_roc_auc'], rfc_scores['train_roc_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6268e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_classifier(X_train, y_train, train_subjects):\n",
    "    xgb_grid_pipe = Pipeline([\n",
    "        ('clf', XGBClassifier())])\n",
    "    \n",
    "    param_grid = {'clf__objective': ['binary:logistic'],\n",
    "                  'clf__use_label_encoder': [False],\n",
    "                  'clf__random_state': [5],\n",
    "                  'clf__max_depth':[3],\n",
    "                  'clf__gamma': [3],\n",
    "                  'clf__min_child_weight': [4],\n",
    "                  'clf__reg_lambda': [5],\n",
    "                  'clf__colsample_bytree': [1]\n",
    "                 }\n",
    "    \n",
    "    grid_xgb = GridSearchCV(xgb_grid_pipe, param_grid = param_grid, cv = innercv, verbose = 0, n_jobs=-1, scoring= 'roc_auc') \n",
    "    grid_xgb.fit(X_train, y_train, groups=train_subjects)\n",
    "\n",
    "    print('Best estimator: ', grid_xgb.best_estimator_)\n",
    "\n",
    "    xgb_pipe = grid_xgb.best_estimator_\n",
    "    \n",
    "    scores = cross_validate(xgb_pipe, X_train, y_train, groups=train_subjects, scoring=scoring, cv=innercv, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "    print('Cross-Validation Evaluation Scores')\n",
    "\n",
    "    print('Train AUC:', 'mean-',np.mean(scores['train_roc_auc']), 'std dev-', np.std(scores['train_roc_auc']))\n",
    "    print('AUC:', 'mean:',np.mean(scores['test_roc_auc']), 'std dev:', np.std(scores['test_roc_auc']))\n",
    "    print('Sensitivity:', 'mean:',np.mean(scores['test_sensitivity']), 'std dev:', np.std(scores['test_sensitivity']))\n",
    "    print('Specificity:', 'mean:',np.mean(scores['test_specificity']), 'std dev:', np.std(scores['test_specificity']))\n",
    "    return grid_xgb.best_params_, grid_xgb.best_estimator_, scores['test_roc_auc'], scores['train_roc_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7ffb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nestedCV(Model, X_outer, y_outer, outer_subjects):\n",
    "    allTestScores = []\n",
    "    allTrainScores = []\n",
    "    allCVScores = []\n",
    "    \n",
    "    for fold, (train_idxs, test_idxs) in enumerate(outercv.split(X_outer, y_outer, outer_subjects)):\n",
    "        # Split into train and test\n",
    "        print(\"\\nREPEAT: \", fold // N_SPLITS_O + 1)\n",
    "        print(\"FOLD: \", fold % N_SPLITS_O + 1)\n",
    "        X_train = X_outer.iloc[train_idxs]\n",
    "        y_train = y_outer.iloc[train_idxs]\n",
    "        X_test = X_outer.iloc[test_idxs]\n",
    "        y_test = y_outer.iloc[test_idxs]\n",
    "        train_subjects = [outer_subjects[i] for i in train_idxs]\n",
    "        \n",
    "        # classification\n",
    "        print(\"\\nMODEL FITTING\")\n",
    "        model_best_param, best_model, model_cv_scores, model_train_scores = Model(X_train, y_train, train_subjects)\n",
    "        allTrainScores.append(np.mean(model_train_scores))\n",
    "        allCVScores.append(np.mean(model_cv_scores))\n",
    "        \n",
    "        # prediction\n",
    "        print(\"\\nINFERENCE\")\n",
    "        auc_score = roc_auc_score(y_test, best_model.predict(X_test))\n",
    "        allTestScores.append(auc_score)\n",
    "        print(\"\\nTEST SCORE: \", auc_score)\n",
    "    \n",
    "    # Calculate averages for each repeat\n",
    "    sumRepeat = 0\n",
    "    for i in range(len(allTestScores)):\n",
    "        sumRepeat += allTestScores[i]\n",
    "        if (i + 1) % N_SPLITS_O == 0:\n",
    "            print(\"\\nAVERAGE TEST SCORE FOR REPEAT {}: {} \".format(i // N_SPLITS_O + 1, sumRepeat/N_SPLITS_O))\n",
    "            sumRepeat = 0\n",
    "    \n",
    "    print(\"\\nAVERAGE TRAIN SCORE ACROSS ALL FOLDS: \", np.mean(allTrainScores))\n",
    "    print(\"\\nAVERAGE BEST CV SCORE ACROSS ALL FOLDS: \", np.mean(allCVScores))\n",
    "    print(\"AVERAGE TEST SCORE ACROSS ALL FOLDS: \", np.mean(allTestScores))\n",
    "    print(\"STD TEST SCORE ACROSS ALL FOLDS: \", np.std(allTestScores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9260c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nestedCV_selection(Model, X_outer, y_outer, outer_subjects):\n",
    "    allTestScores = []\n",
    "    allTrainScores = []\n",
    "    allCVScores = []\n",
    "    allPreFSScores = []\n",
    "    \n",
    "    for fold, (train_idxs, test_idxs) in enumerate(outercv.split(X_outer, y_outer, outer_subjects)):\n",
    "        # Split into train and test\n",
    "        print(\"\\nREPEAT: \", fold // N_SPLITS_O + 1)\n",
    "        print(\"FOLD: \", fold % N_SPLITS_O + 1)\n",
    "        X_train = X_outer.iloc[train_idxs]\n",
    "        y_train = y_outer.iloc[train_idxs]\n",
    "        X_test = X_outer.iloc[test_idxs]\n",
    "        y_test = y_outer.iloc[test_idxs]\n",
    "        train_subjects = [outer_subjects[i] for i in train_idxs]\n",
    "        \n",
    "        # feature selection (select top 20 features)\n",
    "        print(\"FEATURE SELECTION\\n\")\n",
    "        lasso_best_param, lasso_best_model, lasso_scores = Lasso(X_train, y_train, train_subjects)\n",
    "        allPreFSScores.append(np.mean(lasso_scores))\n",
    "        coefficients = lasso_best_model.named_steps['clf'].coef_\n",
    "        importance = np.abs(coefficients).reshape(-1)\n",
    "        posImportance = importance[importance > 0]\n",
    "        sortedImp = np.sort(posImportance)\n",
    "        top20Features = sortedImp[-21:-1]\n",
    "        selected = []\n",
    "        for i in top20Features:\n",
    "            selected.append(feature_names[list(importance).index(i)])\n",
    "        X_train_sel = X_train[selected]\n",
    "        X_test_sel = X_test[selected]\n",
    "        print(selected)\n",
    "        print(len(selected))\n",
    "\n",
    "        # classification\n",
    "        print(\"MODEL FITTING AND INFERENCE\\n\")\n",
    "        model_best_param, best_model, model_cv_scores, model_train_scores = Model(X_train_sel, y_train, train_subjects)\n",
    "        allTrainScores.append(np.mean(model_train_scores))\n",
    "        allCVScores.append(np.mean(model_cv_scores))\n",
    "        \n",
    "        # prediction\n",
    "        auc_score = roc_auc_score(y_test, best_model.predict(X_test_sel))\n",
    "        allTestScores.append(auc_score)\n",
    "        print(\"\\nTEST SCORE: \", auc_score)\n",
    "        \n",
    "    # Calculate averages for each repeat\n",
    "    sumRepeat = 0\n",
    "    for i in range(len(allTestScores)):\n",
    "        sumRepeat += allTestScores[i]\n",
    "        if (i + 1) % N_SPLITS_O == 0:\n",
    "            print(\"\\nAVERAGE TEST SCORE FOR REPEAT {}: {} \".format(i // N_SPLITS_O + 1, sumRepeat/N_SPLITS_O))\n",
    "            sumRepeat = 0\n",
    "\n",
    "    print(\"\\nAVERAGE PRE-FS SCORE ACROSS ALL FOLDS: \", np.mean(allPreFSScores))\n",
    "    print(\"\\nAVERAGE TRAIN SCORE ACROSS ALL FOLDS: \", np.mean(allTrainScores))\n",
    "    print(\"\\nAVERAGE CV SCORE ACROSS ALL FOLDS: \", np.mean(allCVScores))\n",
    "    print(\"AVERAGE TEST SCORE ACROSS ALL FOLDS: \", np.mean(allTestScores))\n",
    "    print(\"STD TEST SCORE ACROSS ALL FOLDS: \", np.std(allTestScores))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75967757",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deabad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from keras.layers.core import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adadelta, Adam\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.metrics import AUC\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping,CSVLogger\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a72f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clf(input_dim, layer1, layer2, layer3, dropout, learning_rate):\n",
    "    # creating the layers of the NN\n",
    "    input_img = Input(shape=(input_dim,))\n",
    "    deep = Dense(units=layer1, activation='relu')(input_img)\n",
    "    deep = Dropout(dropout)(deep)\n",
    "    deep = BatchNormalization()(deep)\n",
    "    deep = Dense(layer2, activation='relu')(deep)\n",
    "    deep = Dropout(dropout)(deep)\n",
    "    deep = BatchNormalization()(deep)\n",
    "    deep = Dense(layer3, activation='relu')(deep)\n",
    "    deep = Dropout(dropout)(deep)\n",
    "    deep = BatchNormalization()(deep)\n",
    "    outlayer = Dense(1, activation='sigmoid')(deep)\n",
    "    model = Model(input_img, outlayer)\n",
    "    model.compile(loss=['binary_crossentropy'], metrics=[AUC()],#, 'sparse_categorical_accuracy'\n",
    "                        optimizer= Adam(learning_rate=learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4611fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(X_train, y_train, train_subjects, fold):\n",
    "    for (train_idxs, test_idxs) in innercv.split(X_train, y_train, train_subjects):\n",
    "        X_train_inner = X_train.iloc[train_idxs]\n",
    "        y_train_inner = y_train.iloc[train_idxs]\n",
    "        X_eval = X_train.iloc[test_idxs]\n",
    "        y_eval = y_train.iloc[test_idxs]\n",
    "        break\n",
    "    \n",
    "    print(\"GETTING LASSO IMPORTANCE SCORES\\n\")\n",
    "    lasso= Lasso(X_train_inner, y_train_inner)\n",
    "    coefficients = lasso.named_steps['clf'].coef_\n",
    "    importance = np.abs(coefficients).reshape(-1)\n",
    "    selected = np.array(feature_names)[importance > 0]\n",
    "    print(selected)\n",
    "    X_train_sel = X_train_inner[selected]\n",
    "    X_eval_sel = X_eval[selected]\n",
    "    \n",
    "    input_dim = len(X_train_sel.columns)\n",
    "    output_dim = 1\n",
    "    \n",
    "    params={'input_dim': [input_dim],\n",
    "            'layer1': [256],\n",
    "            'layer2': [128],\n",
    "            'layer3': [64],\n",
    "            'dropout': [0.2],\n",
    "            'learning_rate': [1e-4]\n",
    "    }\n",
    "\n",
    "        \n",
    "    checkpoint_filepath = os.path.join(model_path, 'best_model_weights_{}.hdf5'.format(fold))\n",
    "    model_checkpoint_callback = tensorflow.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1)\n",
    "    \n",
    "    model = KerasClassifier(model=build_clf,input_dim=input_dim, layer1=256,\n",
    "                            layer2=128, layer3=64, dropout=0.2, learning_rate=1e-4,\n",
    "                            callbacks=[model_checkpoint_callback])\n",
    "    \n",
    "    history = model.fit(X_train_sel, y_train_inner,\n",
    "                    epochs=200, #int(params['n_epochs']),\n",
    "                    batch_size=64,\n",
    "                    shuffle=True,\n",
    "                    verbose=2,\n",
    "                    validation_data=(X_eval_sel, y_eval))\n",
    "        \n",
    "    best_model = build_clf(input_dim=input_dim, layer1=256,\n",
    "                            layer2=128, layer3=64, dropout=0.2, learning_rate=1e-4)\n",
    "    best_model.load_weights(checkpoint_filepath)\n",
    "    \n",
    "    return best_model, selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb993b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def nestedCV_MLP(X_outer, y_outer, outer_subjects):\n",
    "    allTrainScores = []\n",
    "    allTestScores = []\n",
    "    selectedList = []\n",
    "      \n",
    "    #normalize data\n",
    "    sc = StandardScaler()\n",
    "    X_outer_scaled = pd.DataFrame(sc.fit_transform(X_outer))\n",
    "    X_outer_scaled.columns = X_outer.columns\n",
    "\n",
    "    for fold, (train_idxs, test_idxs) in enumerate(outercv.split(X_outer, y_outer, outer_subjects)):\n",
    "        print(\"\\nREPEAT: \", fold // N_SPLITS_O + 1)\n",
    "        print(\"FOLD: \", fold % N_SPLITS_O + 1)\n",
    "        \n",
    "        tensorflow.keras.backend.clear_session()\n",
    "        \n",
    "        # train test split\n",
    "        X_train = X_outer_scaled.iloc[train_idxs]\n",
    "        y_train = y_outer.iloc[train_idxs]\n",
    "        X_test = X_outer_scaled.iloc[test_idxs]\n",
    "        y_test = y_outer.iloc[test_idxs]\n",
    "        train_subjects = [outer_subjects[i] for i in train_idxs]\n",
    "        \n",
    "        print(\"\\nFEATURE SELECTION\")\n",
    "        sel_feat = []\n",
    "        importanceSum = np.zeros(len(feature_names))\n",
    "        B = 500\n",
    "        for i in range(B):\n",
    "            bootstrap = np.random.choice(len(X_train), len(X_train))\n",
    "            X_sample = X_train.iloc[bootstrap]\n",
    "            y_sample = y_train.iloc[bootstrap]\n",
    "            lasso= Lasso(X_sample, y_sample)\n",
    "            coefficients = lasso.named_steps['clf'].coef_.reshape(-1)\n",
    "            importanceSum += coefficients\n",
    "#             selected = np.array(feature_names)[importance > 0]\n",
    "#             sel_feat.append(selected)\n",
    "#             print(selected)\n",
    "        bootstrap_coef = np.abs(importanceSum / B)\n",
    "        selected = np.array(feature_names)[bootstrap_coef > 0.1]\n",
    "        print(selected)\n",
    "#         similarity = []\n",
    "#         for i in range(len(sel_feat)):\n",
    "#             for j in range(i + 1, len(sel_feat)):\n",
    "#                 score = jaccard_similarity(sel_feat[i], sel_feat[j])\n",
    "#                 similarity.append(score)\n",
    "#                 print(score)\n",
    "#         print(\"MEAN SCORE: \", np.mean(similarity))\n",
    "        selectedList.append(selected)\n",
    "        X_train_sel = X_train[selected]\n",
    "        X_test_sel = X_test[selected]\n",
    "        \n",
    "#         print(\"\\nMODEL BUILDING AND FITTING\")\n",
    "#         input_dim = len(X_train_sel.columns)\n",
    "        \n",
    "#         checkpoint_filepath = os.path.join(model_path, 'best_model_weights_{}.hdf5'.format(fold))\n",
    "#         model_checkpoint_callback = tensorflow.keras.callbacks.ModelCheckpoint(\n",
    "#             filepath=checkpoint_filepath,\n",
    "#             save_weights_only=True,\n",
    "#             monitor='val_auc',\n",
    "#             mode='max',\n",
    "#             save_best_only=True,\n",
    "#             verbose=0)\n",
    "    \n",
    "#         model = KerasClassifier(model=build_clf,input_dim=input_dim, layer1=64,\n",
    "#                             layer2=32, layer3=16, dropout=0.25, learning_rate=5e-4,\n",
    "#                             callbacks=[model_checkpoint_callback])\n",
    "        \n",
    "#         history = model.fit(X_train_sel, y_train,\n",
    "#                     epochs=200, #int(params['n_epochs']),\n",
    "#                     batch_size=64,\n",
    "#                     shuffle=True,\n",
    "#                     verbose=0,\n",
    "#                     validation_data=(X_test_sel, y_test))\n",
    "        \n",
    "#         # plot loss during training\n",
    "#         plt.subplot(211)\n",
    "#         plt.title('Loss')\n",
    "#         plt.plot(history.history_['val_loss'], label='valid')\n",
    "#         plt.plot(history.history_['loss'], label='train')\n",
    "#         plt.legend()\n",
    "#         # plot accuracy during training\n",
    "#         plt.subplot(212)\n",
    "#         plt.title('Accuracy')\n",
    "#         plt.plot(history.history_['val_auc'], label='valid')\n",
    "#         plt.plot(history.history_['auc'], label='train')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "        \n",
    "#         best_model = build_clf(input_dim=input_dim, layer1=64,\n",
    "#                                 layer2=32, layer3=16, dropout=0.25, learning_rate=5e-4)\n",
    "#         best_model.load_weights(checkpoint_filepath)\n",
    "        \n",
    "        \n",
    "#         print(\"\\nINFERENCE\")\n",
    "#         # prediction\n",
    "#         train_auc_score = roc_auc_score(y_train, best_model.predict(X_train_sel, batch_size=32))\n",
    "#         print(train_auc_score)\n",
    "#         allTrainScores.append(train_auc_score)\n",
    "#         test_auc_score = roc_auc_score(y_test, best_model.predict(X_test_sel, batch_size=32))\n",
    "#         print(test_auc_score)\n",
    "#         allTestScores.append(test_auc_score)\n",
    "#         print(\"\\nTEST SCORE: \", test_auc_score)\n",
    "        \n",
    "    print(\"\\nAVERAGE TRAIN SCORE ACROSS ALL FOLDS: \", np.mean(allTrainScores))\n",
    "    print(\"\\nSTD TRAIN SCORE ACROSS ALL FOLDS: \", np.std(allTrainScores))\n",
    "    print(\"AVERAGE TEST SCORE ACROSS ALL FOLDS: \", np.mean(allTestScores))\n",
    "    print(\"STD TEST SCORE ACROSS ALL FOLDS: \", np.std(allTestScores))\n",
    "    return selectedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ddc2fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selectedList = nestedCV_MLP(X,  y, allSubjects)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
